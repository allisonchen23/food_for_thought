<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/food_for_thought/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/food_for_thought/" rel="alternate" type="text/html" /><updated>2019-09-06T21:20:52-05:00</updated><id>http://localhost:4000/food_for_thought/feed.xml</id><title type="html">Thoughts for Food</title><subtitle>Welcome to my blog! I'll be posting basically random thoughts and updates on any current projects/work I'll be doing.</subtitle><entry><title type="html">Eat to Beat Disease: a MUST READ</title><link href="http://localhost:4000/food_for_thought/eat-to-beat-disease/" rel="alternate" type="text/html" title="Eat to Beat Disease: a MUST READ" /><published>2019-09-05T08:25:40-05:00</published><updated>2019-09-05T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/eat-to-beat-disease</id><content type="html" xml:base="http://localhost:4000/food_for_thought/eat-to-beat-disease/">&lt;p&gt;The other day I was at the library, looking for a quick read before I head off to school in a week, and I spotted a thick book with color foods on the front titled &lt;em&gt;Eat to Beat Disease&lt;/em&gt;. ‘This sounds interesting!’ I thought to myself, and as I was in a hurry to leave, I quickly checked it out. That night, I started reading after dinner and literally could not stop. This book is everything I’ve been looking for! And I’m not exaggerating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/food/eat_to_beat.jpg&quot; alt=&quot;Selfie with my favorite new book&quot; class=&quot;images third&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;caption&quot;&gt;Here it is! My favorite new read&lt;/p&gt;

&lt;p&gt;For the longest time, I wanted a resource (a collection of journal articles, podcasts, book, &lt;em&gt;anything!&lt;/em&gt;) that explained why some foods are healthy and others aren’t with scientific backing, and this book &lt;span class=&quot;standOut&quot;&gt;does it all&lt;/span&gt;. It explains why certain foods are good for you beyond the scope of how full it makes you or whether the calories are “worth it”. In fact, I’m currently about halfway through the book, and the author, Dr. William Li, has barely even talked about calories! WAKE UP AMERICA! Calories are &lt;strong&gt;not&lt;/strong&gt; the only measure of health!&lt;/p&gt;

&lt;p&gt;As a quick sneak peek, Dr. Li first explains five of the body’s amazing defense mechanisms against diseases including cancer, diabetes, heart disease, and many others that seem to only be treatable by modern medicine. The five mechanisms are summarized as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Angiogenesis: building blood vessels to various parts of the body&lt;/li&gt;
  &lt;li&gt;Regeneration: using stem cells to repair injuries&lt;/li&gt;
  &lt;li&gt;Our gut’s microbiome: the good bacteria living inside of us&lt;/li&gt;
  &lt;li&gt;DNA Projection and Repair: affecting expression of certain genes&lt;/li&gt;
  &lt;li&gt;Immunity: using our immune system to build resistance against certain diseases/invaders.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Except, read the book for a much more thorough and in depth explanation of the mechanisms; this is barely the tip of the iceberg! Then, Dr. Li goes on to explain which foods affect each of the five mechanisms and guess what! They’re mostly foods that you’re probably already eating! He backs up each food listed with scientific evidence, whether from clinical trials or experiments done in a lab setting, and usually uses more than 1 experiment as an example.&lt;/p&gt;

&lt;p&gt;For me personally, before picking up this book, I kind of judged foods on if they were healthy or not based on if they had a lot of fiber and would fill me up while minimizing the number of calories I consumed. However, this is such a black and white scale for a field as colorful as a rainbow, almost literally! There’s so much more to health than just digestion rate. For example, it’s known that cheese is pretty calorie/energy dense and contains quite a bit of saturated fat, so I tended to avoid it as much as I can. However, cheese (especially European cheese) has a lot of probiotics and prebiotics (good bacteria and food for the bacteria in our microbiome, respectively) that will improve our overall health! In moderation, of course, cheese is really beneficial!&lt;/p&gt;

&lt;p&gt;I’m already looking at the foods I eat and making a mental note of which defense mechanism I’m improving by eating this, but the list is so long, I’ll definitely have to give &lt;em&gt;Eat to Beat Disease&lt;/em&gt; another read to understand more!&lt;/p&gt;

&lt;p&gt;As summer turns into Fall and we’re stuck inside anyways, &lt;strong&gt;definitely&lt;/strong&gt; put this on your read list!&lt;/p&gt;

&lt;p&gt;Note: it is a little scientifically dense, and while Dr. Li does a great job of explaining it, if a concept is new, don’t be afraid to re-read a part a few times or use Google! I know I definitely did :)&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">The other day I was at the library, looking for a quick read before I head off to school in a week, and I spotted a thick book with color foods on the front titled Eat to Beat Disease. ‘This sounds interesting!’ I thought to myself, and as I was in a hurry to leave, I quickly checked it out. That night, I started reading after dinner and literally could not stop. This book is everything I’ve been looking for! And I’m not exaggerating.</summary></entry><entry><title type="html">Sure about sugar?</title><link href="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar.html" rel="alternate" type="text/html" title="Sure about sugar?" /><published>2019-08-27T00:00:00-05:00</published><updated>2019-08-27T00:00:00-05:00</updated><id>http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar</id><content type="html" xml:base="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar.html">&lt;h5 id=&quot;all-sugar-is-bad-right&quot;&gt;All sugar is bad right?&lt;/h5&gt;

&lt;p&gt;Okay that’s a tough question. Sugar by itself is not necessarily bad; it’s a good source of quick energy for our bodies. &lt;em&gt;HOWEVER&lt;/em&gt;, too much sugar, especially in its most processed form can be pretty detrimental to our bodies and lead to weight gain as well as medical issues.&lt;/p&gt;

&lt;h5 id=&quot;how-does-our-body-digest-sugar&quot;&gt;How does our body digest sugar?&lt;/h5&gt;

&lt;p&gt;Basically, sugar will be broken down to glucose, which is the form of energy our cells require, and the glucose gets sent to one of three places:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Our cells, to give them energy&lt;/li&gt;
  &lt;li&gt;Our liver, to store glucose for later in the form of glycogen (which you don’t need to worry about)&lt;/li&gt;
  &lt;li&gt;Our adipose tissue as fat.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our cells prefer to have sugar because they can quickly be broken down into glucose, but our cells can only take so much glucose at a time. Same with our liver. The first two have a capacity of how much glucose gets sent there, but adipose tissue &lt;span class=&quot;standOut&quot;&gt; can take as much glucose as it wants&lt;/span&gt;. That’s where the dangerous part is.&lt;/p&gt;

&lt;p&gt;If our body breaks down too much sugar at once and has too much glucose for our cells and liver, it will send all the excess glucose to turn into fat, which will lead to weight gain.&lt;/p&gt;

&lt;h5 id=&quot;so-we-should-avoid-all-sugar&quot;&gt;So we should avoid all sugar?&lt;/h5&gt;

&lt;p&gt;Not necessarily! The main difference in sugar that is, for example, added in candy and sodas, and sugars in whole fruit is the magical thing called &lt;span class=&quot;standOut&quot;&gt; fiber&lt;/span&gt;. The sugars in fruit are all encased by a lot of fiber, which if you recall, slows down the digestion of food. This means that the sugar in fruit gets slowly broken down and transported to the appropriate location over a longer period of time. If you ate a candy bar, on the other hand, basically all that sugar gets broken down to glucose and taken to the appropriate destination all at once. Sugars in fruit are able to provide your &lt;em&gt;cells&lt;/em&gt; with a more constant form of energy, which also helps you feel fuller and maintain energy.&lt;/p&gt;

&lt;p&gt;In fact, fiber is actually the main difference between white sugar and brown sugar. Although brown sugar is not the same as the sugar we get in whole fruits, it has more fiber compared to white sugar, and thus is marketed as healthier.&lt;/p&gt;

&lt;h5 id=&quot;how-do-we-know-what-has-added-sugar&quot;&gt;How do we know what has added sugar?&lt;/h5&gt;

&lt;p&gt;Ah yes this can be the tricky one. The first place I would check is the nutrition label. Under ‘sugar’, some labels have a row for ‘added sugar’ which is not very desirable. However, for a more thorough check, you must go to the ingredient list. &lt;span class=&quot;standOut&quot;&gt;But sugar is tricky.&lt;/span&gt; After people realized sugar was leading to weight gain and other health complications, many companies became sly about hiding sugar into their products by using different names for sugar but in reality, it just means ‘Added Sugar’. Some common ones to look out for are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The mono and disaccharides (dextrose, fructose, galactose, glucose, lactose, maltose, and sucrose)&lt;/li&gt;
  &lt;li&gt;Cane sugar, evaporated cane juice&lt;/li&gt;
  &lt;li&gt;High Fructose Corn Syrup&lt;/li&gt;
  &lt;li&gt;Corn syrup solids&lt;/li&gt;
  &lt;li&gt;Dextrin&lt;/li&gt;
  &lt;li&gt;Glucose syrup solids&lt;/li&gt;
  &lt;li&gt;Maltodextrin&lt;/li&gt;
  &lt;li&gt;Agave Nectar/Syrup (very similar to honey)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;im-never-eating-sweets-again-right&quot;&gt;I’m never eating sweets again, right?&lt;/h5&gt;

&lt;p&gt;I’d say enjoy in moderation! It’s very difficult to cut out something entirely from your diet because you’ll probably start to crave it even more, causing a binge eating session. It’s okay to enjoy that sweetness of a candy bar or ice cream once in a while, but be mindful of what’s happening inside your body after you eat it.&lt;/p&gt;</content><author><name></name></author><category term="nutrition" /><summary type="html">All sugar is bad right?</summary></entry><entry><title type="html">Week 9: Wrapping Things Up!</title><link href="http://localhost:4000/food_for_thought/FM-week-9/" rel="alternate" type="text/html" title="Week 9: Wrapping Things Up!" /><published>2019-08-26T20:31:00-05:00</published><updated>2019-08-26T20:31:00-05:00</updated><id>http://localhost:4000/food_for_thought/week-9</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-9/">&lt;h3 id=&quot;monday-august-26-2019&quot;&gt;Monday August 26, 2019&lt;/h3&gt;

&lt;p&gt;Happy Monday! This past weekend was so relaxing! It was kind of hard to get up for work today haha. And I faced a bit of a surprise when I arrived at the museum because my badge didn’t work anymore! I was supposed to end the internship last week, but we extended it for one week and we didn’t let Access Control know… oopsies&lt;/p&gt;

&lt;p&gt;It’s all fixed now, don’t worry :D&lt;/p&gt;

&lt;p&gt;After that mini roadblock, I continued to examine the models and make changes to improve them. In my model training to find the differences between Lycopodium and Selaginella, some folds did really well and training accuracy reached 90% while validation accuracy lagged behind, but in other folds, the training &lt;em&gt;and&lt;/em&gt; validation accuracy both plummeted to around 50% by the 10-15 epoch and just couldn’t recoveer. In all epochs, however, loss was constantly decreasing. I’m not really sure what’s happening there and why it would happen on some folds and not others… maybe there’s a few images that are just really throwing the model off? Although I did look through them manually before. Perhaps it’s worth another look.&lt;/p&gt;

&lt;p&gt;I first looked into doing image augmentation, but I originally thought that it would result in the model training on &lt;em&gt;more&lt;/em&gt; images than you have every epoch. Turns out, it just trains on a smaller sample, but each epoch it uses &lt;em&gt;randomly altered&lt;/em&gt; images that are different to the model. It may be useful in another application (like the frullania), but currently when we have so many images already, I don’t think we need it.&lt;/p&gt;

&lt;p&gt;I tried playing around with the regularizers today in the Dense and Convolutional-2D (Conv2D) layers, but most combinations I tried resulted in the model overfitting very early and the training and validation accuracy would begin to decrease by the 6-7th epoch. (To see what changes I made, check out &lt;a href=&quot;https://docs.google.com/spreadsheets/d/15972K_rdv3zpnvnV--wSaTpiK0jnVDkBzhUWjre5BLg/edit?usp=sharing&amp;quot;&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;this doc&lt;/a&gt;. The model started to learn again pretty well when I kept activity regularizers in the Dense layers, despite there not being an exact equivalent in the Smithsonian Methematica Model.&lt;/p&gt;

&lt;p&gt;We’ll see what happens tomorrow morning!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;p&gt;PS, Matt wants me to try running the model again on the two frullania species again but the images being from farther away. I want to look into bootstrapping for this and hopefully synthetically expand our data set.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-28-2019&quot;&gt;Tuesday, August 28, 2019&lt;/h3&gt;

&lt;p&gt;First thing this morning, I checked on the training of the model on Lycopodium and Selaginella. For the most part, it was okay; there was still a bit of overfitting, but the trend of validation accuracy seemed to follow training accuracy, like in the graph below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/val_accuracy_1.png&quot; alt=&quot;Slightly overfit accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, on other folds, we got something disastrous like this:
&lt;img src=&quot;/assets/images/blog/fm/val_accuracy_8.png&quot; alt=&quot;Dropped accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m not entirely sure why on most folds, the model seems to be training well, while on others, it’s completely unable to after a certain amount of time. I don’t think it’s overfitting because even the training accuracy decreased. Well, tomorrow, I’m meeting with Dr. Iacobelli, the computer science professor at Northeastern Illinois University, so hopefully he’ll be able to offer more insight.&lt;/p&gt;

&lt;p&gt;Additionally, I received new images of the frullania species today. These ones are more zoomed in and hopefully will allow the model to pick up on more details in the differences between the two plants. I implemented some bootstrapping to train the model because we only had a little less than 300 images which I’m not sure is enough to be able to completely train the model on. I left it going overnight, so I’ll be able to check in on Thursday.&lt;/p&gt;

&lt;p&gt;At this point, most of my big machine learning changes are wrapping up and I’ve been working on documentation, documentation, and documentation! Especially since I was pretty much the only one working on this project this summer, it’s &lt;strong&gt;extremely&lt;/strong&gt; important that someone could study my work for a bit and pick up where I left off.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-28-2019&quot;&gt;Wednesday August 28, 2019&lt;/h3&gt;

&lt;p&gt;This morning I met with Dr. Iacobelli, Beth, and Dr. Campbell (unfortunately Matt coudln’t make it) We updated Dr. Iacobelli and here are his responses/advice. Regarding the random dropped accuracy I wrote about yesterday, he recommends that we meticulously check the code first. There may a bug (such as divide by 0) that’s throwing everything off. Additionally, it simply may be lack of processing power in the computer causing a data overflow.&lt;/p&gt;

&lt;p&gt;Another thing he noticed is that the images we’re using from the Field Musuem are &lt;em&gt;not&lt;/em&gt; consistent in the herbarium sheet layout. Some have labels in the upper right corner, some are lower left, some have specimen in the middle, some have specimen in the lower right corner. The inconsistencies in layout may be providing too much noise for the model, so he recommends that we try a smaller sample of images that have a more consistent layout an see if there is an improvement in performance.&lt;/p&gt;

&lt;p&gt;Additionally, there is a computer at NEIU with 64 GB RAM that Beth is going to be trying our model with the Smithsonian images (which I didn’t know we had!) and we found out there is a computer at the museum with &lt;span class=&quot;standOut&quot;&gt; one terabyte of RAM. I repeat, of RAM. &lt;/span&gt; I almost fainted when I heard that.&lt;/p&gt;

&lt;p&gt;Although I’m wrapping up this week, I’m excited for Beth to be able to try things out!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-29-2019&quot;&gt;Thursday August 29, 2019&lt;/h3&gt;

&lt;p&gt;When I came in this morning, the model that was training on images of the Adiantum and Blechnum genera finished training, so I created a script to test it. Since the model really consists of 10 trained models (given that we used 10 fold cross validation), I had to take that into account. One way to do this would be see which prediction output each model gave and the final prediction would be what the majority of the models output. However, this poses two possible issues:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What if 5 models say class A and 5 say class B? There wouldn’t be an odd number in this case to be the tie breaker&lt;/li&gt;
  &lt;li&gt;If one model is 99% sure that an image is class A but another model is only 50% sure that it’s class A, using the method described above would give each of these equal weights when it maybe shouldn’t.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To overcome this, I decided to take a step back and look at the probability outcomes that are used to determine each model’s final answer. Basically, the final layer of the model outputs two numbers: the probability that an image is one class or another. So what I did was I kept track of each of these probabilities and for each image, added up all the models’ class A probability and class B probability separately. The final answer would be whichever summed probability is higher. This way, the “confidence” of each of the 10 models is taken into account.&lt;/p&gt;

&lt;p&gt;Honestly, that didn’t take me very long today, so when I tested the Adiantum/Blechnum model, we got a whooping &lt;span class=&quot;standOut&quot;&gt;97.8% accuracy!!&lt;/span&gt; It was tested on 400 new images and this is the best I’ve ever gotten :)))&lt;/p&gt;

&lt;p&gt;This is good news because it means the model can work! We just need to tweak values and perhaps look at cleaning our input images for Lycopodium and Selaginella.&lt;/p&gt;

&lt;p&gt;The rest of the day was mainly spent documenting my work. I explained to Matt how my Google Drive was organized and how to use Github as well as documented within the Drive where everything is.&lt;/p&gt;

&lt;p&gt;I did, however, forget to upload my final cross-validation testing file to Github, which is what I’ll do first thing tomorrow morning.&lt;/p&gt;

&lt;p&gt;Last day tomorrow! What a melancholy feeling…&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-30-2019&quot;&gt;Friday August 30, 2019&lt;/h3&gt;

&lt;p&gt;Today was spent mainly double checking my documentation on Github and Google Drive. I called Beth and we went over each file in Github and she seems confident on where I’m leaving off, which is great! I’m so excited for her to continue this project and be able to run the model on a computer that has enough processing power.&lt;/p&gt;

&lt;p&gt;Furthermore, Matt worked on getting the images contrasted a bit more yesterday; now the background of the images are very white compared to the specimen in focus and it helped get rid of some noise. We only have a little less than 150 of either group, but we ran the model for 50 epochs and we actually got some results! Although overfitting still occurred, the validation accuracy definitely grew as the number of epochs increased! This is a great sign! It means the machine is able to generalize what it’s learning on the training data onto new images.&lt;/p&gt;

&lt;p&gt;Since we didn’t run for very long and we don’t have many images, we were able to start another run before the end of the day. Matt went through the images and deleted any that were actually the wrong specimen or weren’t sterile as this type of noise could be hurting the model’s performance. Then, before I left, I ran two models: one with the same exact specs as the last run, just with the new images, and one with the new images &lt;em&gt;and&lt;/em&gt; more epochs. Although I won’t be able to see the results, Matt will be able to check it out and report back!&lt;/p&gt;

&lt;p&gt;And that’s a wrap :) although I’m leaving, I know this project is going to go far in the hands of Beth, Matt, and Dr. Iacobelli. I’m excited to see where they are able to take this application of computer science into botany and am truly grateful for this opportunity!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 26, 2019</summary></entry><entry><title type="html">Becoming Keen on Quinoa</title><link href="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa.html" rel="alternate" type="text/html" title="Becoming Keen on Quinoa" /><published>2019-08-20T00:00:00-05:00</published><updated>2019-08-20T00:00:00-05:00</updated><id>http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa</id><content type="html" xml:base="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa.html">&lt;h5 id=&quot;sooo-what-is-quinoa&quot;&gt;Sooo what is quinoa?&lt;/h5&gt;

&lt;p&gt;Quinoa (pronounced keen-wah) is a type of grain that is in my opinion both delicious and extremely nutritious. Uncooked, they look like little seeds, normally yellow or brown, but after boiling, they become soft and fluffy. In addition to being an excellent source of carbs and energy, they have a surprising amount of protein and are even&lt;span class=&quot;standOut&quot;&gt; a complete protein&lt;/span&gt;! Meaning quinoa contains &lt;strong&gt;all nine&lt;/strong&gt; essential amino acids.&lt;/p&gt;

&lt;h5 id=&quot;but-why-is-it-better-for-me-than-flour-or-rice&quot;&gt;But why is it better for me than flour or rice?&lt;/h5&gt;

&lt;p&gt;For a few reasons, actually. First quinoa is not as processed as flour and thus retains more of its vitamin and minerals as well as fiber. Compared to rice, quinoa’s glycemic index is significantly lower (53 compared to 73 of white rice) which means that it does not produce as big of an insulin spike. (Which is good! We don’t want to unnecessarily tire out our pancreas.) The main reason for this is &lt;span class=&quot;standOut&quot;&gt; fiber&lt;/span&gt;!!&lt;/p&gt;

&lt;p&gt;Recall that fiber is essentially a food’s “protective barrier” to being digested. But of course, our digestive system always overcomes it. Fiber itself is undigestable and slows down our body’s ability to digest the food it encases. This may sounds bad, but it’s actually extremely good, especially for individuals who are wary of caloric intake. The fiber helps us eat the same number of calories, but feel fuller for longer, reducing the cravings we often feel after eating pasta or other refined grains.&lt;/p&gt;

&lt;p&gt;If you take away anything from this blog post, I hope it’s that &lt;span class=&quot;standOut&quot;&gt; quinoa = fiber! &lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&quot;what-do-you-even-eat-it-with&quot;&gt;What do you even eat it with?&lt;/h5&gt;

&lt;p&gt;I love quinoa as a rice/grain substitute! Maybe it’s the Asian side in me, but I’ll often have a bowl of quinoa with my dinner or lunch just instead of rice. Additionally, it’s really good in salads and or various types of bowls. It’s really quick and easy to cook a bunch of quinoa at once and refrigerate; it can probably last a good week or so!&lt;/p&gt;

&lt;h5 id=&quot;im-a-broke-college-student-how-can-i-afford-this&quot;&gt;I’m a broke college student, how can I afford this?&lt;/h5&gt;

&lt;p&gt;Admittedly, quinoa can be more expensive than some easier foods, such as pasta or rice. But remember, nutrition is about a long run investment. Although you may be spending a bit more now, you’re treating your body better (which in itself is priceless!) and maybe even saving money on future medical bills. Plus, a small bag of quinoa actually makes quite a lot, I think you’d be surprised :)&lt;/p&gt;

&lt;h5 id=&quot;how-do-you-cook-quinoa&quot;&gt;How do you cook quinoa?&lt;/h5&gt;

&lt;p&gt;Easy! Simply put about 1 cup of quinoa and 1 cup of water in a pot and bring to a boil. If you’re cooking less, I would put a little more water, about a 1:1.25 quinoa to water ratio. Once it starts to boil, turn down the heat and cover the pot, allowing the quinoa to simmer. Once most of the water is gone, I would check on it frequently and stir to prevent any quinoa from sticking to the bottom.&lt;/p&gt;

&lt;p&gt;The quinoa is cooked if it looks almost “fluffy” and it shouldn’t be crunchy anymore.&lt;/p&gt;</content><author><name></name></author><category term="nutrition" /><summary type="html">Sooo what is quinoa?</summary></entry><entry><title type="html">Week 8: Implementing Cross Validation</title><link href="http://localhost:4000/food_for_thought/FM-week-8/" rel="alternate" type="text/html" title="Week 8: Implementing Cross Validation" /><published>2019-08-19T08:25:40-05:00</published><updated>2019-08-19T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-8</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-8/">&lt;h3 id=&quot;monday-august-19-2019&quot;&gt;Monday August 19, 2019&lt;/h3&gt;

&lt;p&gt;This morning, Beth came to the Field Museum to work with me on implementing stratified cross validation and ROC testing. Stratified cross validation essentially partitions the data into groups of approximately equal distributions of each class. ROC (stand for Receiving Operator Curve) is a type of graph that can help us quantify how “good” our model is. It uses the percentage of true positives and false positives and the closer the curve hugs the y-axis, the more confident it is. Take a look at the image below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/ROC_curve.png&quot; alt=&quot;ROC Curve&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The straight line represents if it were just up to chance, so if the ROC curve is close to that, it’s bad.&lt;/p&gt;

&lt;p&gt;I got the code to run with stratified cross validation and creating an ROC curve at the end with small amounts of bogus test data, but when I tried to import larger amounts of useful data, the program seems to be crashing and I’m not sure why. I ran the old model again (prior to implementing cross validation) and it seemed to work okay, but in this new script, it feels like everything’s falling apart (rip). Before the program started crashing, it would train but ridiculously overfit on the training data while validation accuracy didn’t increase at all :(&lt;/p&gt;

&lt;p&gt;I’m trying to see if I accidentally changed something when adapting the model, but then the code crashed entirely, freezing the computer.&lt;/p&gt;

&lt;p&gt;This is definitely a rough note to end the day on, but I’m going to try not to let it bother me too much and come in tomorrow ready to problem solve!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-20-2019&quot;&gt;Tuesday August 20, 2019&lt;/h3&gt;

&lt;p&gt;This morning, I was able to fix why the code would crash my computer! When I was using the split function on the stratifiedKFold object and accessing features and labels that made up the training and validation sets, they had to be numpy arrays. My labels data structure was only a list and thus would throw an error and crash it. After making a few more tweaks on random seeds and epochs, aroudn 10:30 AM I started running the 10 fold cross validation and it hasn’t stopped since. I’ve left it on and hope no one messes with it! Tomorrow morning, we’ll be able to check out results and see what changes we can make to improve the model.&lt;/p&gt;

&lt;p&gt;While the model was training, I was tasked with a side task. There’s a bryophyte checklist of the Java region that’s written as a word document. We want to parse the document and store the genus, species, and founder into a CSV. Currently, I’m using a package called python-docx and basic python string manipulation to parse out the desired information. The only downside is this code I’m writing is only specific to this document which of course isn’t as fun in CS.&lt;/p&gt;

&lt;p&gt;Have a great day!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-21-2019&quot;&gt;Wednesday August 21, 2019&lt;/h3&gt;

&lt;p&gt;Today was a &lt;strong&gt;long&lt;/strong&gt; day, oof. To start out, Matt and I talked about the project and thought about why our results weren’t exactly matching that of the Smithsonian. First, and foremost, it’s most likely because of our data that we’re using. The Smithsonian is using images from their own herbarium while we’re using images from the Field Museum herbarium and obviously those will produce different results. Within the family of Lycopodiaceae, the distribution of images across genera are different between their herbarium and ours. Additionally, they were able to use a lot more images due to processing power. I found out today, that using around 4000 images is a lot for the computers we have (i5 Intel Cores with 8 GB RAM). Sometimes the computer just freezes entirely and you have to hard restart it. Thus, I decreased my number of images to around 3300 and it seems to be running okay!&lt;/p&gt;

&lt;p&gt;Additionally, instead of comparing families of plants (Lycopodiaceae and Selaginellaceae), we decided to simplify the inputs to two genera of plants (Lycopodium and Selaginella). Selaginella is the only genus in the family Selaginellaceae (as a reminder the hierarchy goes family &amp;gt; genus &amp;gt; species) and Lycopodium is one of around 8 or so genera in the family Lycopodiaceae. Hopefully this will provide less noise in the first class and make it easier for the machine to detect prominent differences.&lt;/p&gt;

&lt;p&gt;Additionally, just for kicks, Matt wanted me to run the model to compare two new and different genera: adiantum and blechnum. This is because these two families look VERY different to the eye, so the model should do a relatively good job with this. I spent most of today preparing the images to be run on a different computer and started running the model before I left.&lt;/p&gt;

&lt;p&gt;I got in around 8:15 this morning and didn’t leave till around 5:30, I’m going to go sleep a lot now!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-22-2019&quot;&gt;Thursday August 22, 2019&lt;/h3&gt;

&lt;p&gt;I came in this morning and good news, nothing crashed! The models are still chugging along doing their own thing. Something I noticed and continue to notice is that while training accuracy will rise somewhat consistently with each epoch, validation accuracy fluctuates a LOT more, like in the graph below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/8.14.19_12.30.png&quot; alt=&quot;Fluctuating Validation Accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m going to look into this more, but after a quick Google search, it seems one issue may be too much dropout. So after this model is finished training, I’m going to look into making that change (which might end up having to be done tomorrow)!&lt;/p&gt;

&lt;p&gt;The program I started running yesterday with Lycopodium and Selaginella was taking forever… after 18 hours, it only completed 5 folds of the 10 fold cross validation and Beth and I decided that it was too long to realistically experiment. Thus, we decreased the images from 256 x 256 pixels down to 128 x 128 and it seems to be going much faster! Each epoch now takes a little over a minute instead of around 10 minutes.&lt;/p&gt;

&lt;p&gt;Additionally, I’m running a baseline test with the model and images of Frullania rostrata and Frullania coastal. As a reminder, these are two possibly distinct species that we want to gather further evidence that they are different. Although the model isn’t “tuned” to picking up the differences in the Frullania, it’ll be nice to have a baseline essentially and see how to go from there. Beth had also mentioned that there’s a way to see what features each layer of the model is picking up, which would be helpful in our case to see what layers are working for our purpose and what should be changed.&lt;/p&gt;

&lt;p&gt;Take it easy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;h3 id=&quot;friday-august-23-2019&quot;&gt;Friday August 23, 2019&lt;/h3&gt;

&lt;p&gt;Happy Friday! When I came in this morning, all three of my models finished running and here’s a brief summary of the results:&lt;/p&gt;

&lt;p&gt;The Lycopodium/Selaginella results: The validation accuracy followed the training accuracy, but there was still a lot of fluctuation. In my next training, I increased batch size from 32 to 64 and increased epochs to 100. May as well since we have the weekend to run it. Before I left for the day, it finished training 2/10 folds and I noticed in the second fold, by the end neither training nor validation accuracy increased from 50%. This leads me to believe we shouldn’t increase batch size much above 32.&lt;/p&gt;

&lt;p&gt;The Adiantum/Blechnum results: This run was mostly as a baseline to see whichout changing the model, what results did we get. Again, they were pretty similar to before: validation accuracy seemed to follow training accuracy but with a LOT more and larger fluctuations. In this next training session, I decreased the initial dropout layer from 50% to 40% to see if perhaps less measures against overfitting would help.&lt;/p&gt;

&lt;p&gt;The Frullania (coastal/rostrata) results: awful. I really didn’t expect much but what we saw here in all the folds was a classic case of overfitting. The training accuracy kept increasing while validation accuracy remained fluctuating around 50%. To make this work, we would need to reconfigure the layers of the model to “detect” these certain features.&lt;/p&gt;

&lt;p&gt;On a more fun note, today our department had a large rpotluck! There were all sorts of food from Malaysian, Latin American, to Chinese! I brought carrots and hummus and a Chinese Eight Treasures Cake which got really good feedback! I ate literally so much food but it was absolutely amazing :)&lt;/p&gt;

&lt;p&gt;Have a great weekend in this beautiful weather!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 19, 2019</summary></entry><entry><title type="html">Week 7: Improving CNN Training</title><link href="http://localhost:4000/food_for_thought/FM-week-7/" rel="alternate" type="text/html" title="Week 7: Improving CNN Training" /><published>2019-08-12T08:25:40-05:00</published><updated>2019-08-12T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-7</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-7/">&lt;h3 id=&quot;monday-august-12-2019&quot;&gt;Monday August 12, 2019&lt;/h3&gt;
&lt;p&gt;Today was essentially spent just making changes to the architecture/regularizers of my model and training it to hopefully get better results. The main issues I’m facing now is that validation accuracy fluctuates a lot. It’ll grow steadily for a little then drop down to almost 50% and grow a little more (such as in the image below).
&lt;img src=&quot;/assets/images/blog/fm/8.13.19_13.47.png&quot; alt=&quot;Accuracy Graph&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although it took all day because training the model currently takes around an hour, the main things I experimented with were changing the epsilon value (which prevents dividing by 0) and the regularizers. Regularizers essentially penalize larger weights, forcing the parameters to be small. I used L2 Regularization which adds a penalty term to the loss function that’s composed of the sum of the squares of the parameters.&lt;/p&gt;

&lt;p&gt;The validation accuracy is still fluctuating a lot but I just called it a day after working on this for a long time.&lt;/p&gt;

&lt;p&gt;See you tomorrow!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;tuesday-august-13-2019&quot;&gt;Tuesday August 13, 2019&lt;/h3&gt;
&lt;p&gt;Continuing the work of yesterday, I was a little frustrated with why the validation accuracy was so awful. Doing some more comparisons between my model architecture and the Smithsonian, I did notice a discrepancy. In the Smithsonian paper (which as a reminder used Mathematica, not Python) had layers called ‘linear layers’ which simply output weights*inputs+bias. I assumed a Dense layer in Keras did the same, but after doing some research, I found that I was wrong. I added the linear activation function to these layers and the results seemed to be at least a little more consistent in terms of validation accuracy/loss.&lt;/p&gt;

&lt;p&gt;Although it’s still far from perfect, the validation losses and accuracy seem to be following the training loss and accuracy a little more closely. I believe the fluctuations mean that the model is still overfitting to training data and is unable to be consistent with the validation cases. The next step I want to try is incorporating more drop out or possibly increasing regularizers. I do need to be careful with regularizers though; I don’t want to have too much and prevent the model from continuing to learn.&lt;/p&gt;

&lt;p&gt;In addition to using a model to learn between Selaginellaceae and Lycopodiaceae, we want to see if we can apply the same model architecture to two possibly different species of a plant under Frullania. This would be a completely new application and really cool! But still in the process of obtaining images right now. One thing that will be kind of difficult with that is we only have around 100 images of each while with the families I’m working with now, there are thousands and I’m still struggling to get a good model.&lt;/p&gt;

&lt;p&gt;Wish me luck, I’ll need it.&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-14-2019&quot;&gt;Wednesday August 14, 2019&lt;/h3&gt;

&lt;p&gt;This morning was a bit slow because training the model currently takes a very long time, but I added another dropout layer which seems to help with overfitting! Validation accuracy is fluctuating just a little bit less :) Additionally, I found that for some reason, when I end training and resume it again at the same epoch, it produces different results than if I don’t stop. This leads me to believe there is something that I haven’t been able to control in regards to the randomness of the model and it may be causing misleading results. This is something I’d like to look more into.&lt;/p&gt;

&lt;p&gt;Additionally, I received images today of two different species that Dr. Matt von Konrat (aka my boss) has been doing research on. They’re called frullania coastal and frullania rostrata (you may recall what they look like from previous posts about Morphosnake). Right now, we’re trying to build up the amount of evidence to show that these in fact are different species. However, we only have about 90-100 of each species, so I’m currently looking into using image augmentation with Keras to give ourselves more images and prevent overfitting. As a reference for myself, I’m using &lt;a href=&quot;https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/&quot; class=&quot;standOut&quot;&gt;this website&lt;/a&gt; as a reference. I can also look into changing my model to help against overfitting. For example, in the other project, my batch size was 32, but since I only have about 130 images total to train with right now, decreasing my batch size to 8 already improved validation accuracy without sacrificing too much time.&lt;/p&gt;

&lt;p&gt;This is something new and exciting to me! Hopefully we can get some results by early next week :)&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-july-15-2019&quot;&gt;Thursday July 15, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I worked on a data summary script to finalize a project that the high school interns worked on. As a reminder, their project was a community science activity on Zooniverse that allowed people to examine fern specimen. We wanted to see in general how close the community user responses were to an expert response (done by our very own Dr. Matt von Konrat). I worked on a script that took the exported data from Zooniverse and parsed through it. Although it wasn’t difficult, it took a while because Jessica (another intern!) and I kept chatting (oops!)&lt;/p&gt;

&lt;p&gt;After that, I started working back on the machine learning project. Last night, I called with Dr. Francisco Iacobelli, a computer science professor at Northeastern Illinois University. We talked about implementing k-fold Cross Validation methods to test the robustness of a model. The idea is that you take all your images and split it into 10 groups randomly. Choose one group to be the validation data and the successive group to be the testing data. The remaining data acts as training data. Train the model and save it, then repeat so all groups get a chance to be the testing data. In the end you have 10 trained models that really make up &lt;strong&gt;the model&lt;/strong&gt;. This makes a lot more sense to me after listening to Dr. Iacobelli talk about it; I had seen it online but for some reason it didn’t really stick until now.&lt;/p&gt;

&lt;p&gt;By the end of the day, I think I almost finished implementing the cross validation! I’m currently in the process of writing the loop that partitions the training, testing, and validation data, and after that, I just have to run and save the model for each group.&lt;/p&gt;

&lt;p&gt;I won’t be at the museum tomorrow, but I’ll try to get some work done on my own, mostly just documentation so I don’t have to do that later.&lt;/p&gt;

&lt;p&gt;Happy Thursday everyone!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-16-2019&quot;&gt;Friday August 16, 2019&lt;/h3&gt;

&lt;p&gt;Today I didn’t go to the museum, I had a meeting at Northeastern Illinois University with our grad student Beth, Dr. Campbell (a biology professor who’s been working with some of our summer interns), and Jose. We basically just updated everyone else where we are on the project and Beth and I communicated a bit more on how we can work together. She’s been pretty busy with other projects at the same time, so this was a good opportunity for us to touch base.&lt;/p&gt;

&lt;p&gt;After the meeting, it wasn’t worth it for me to go back to the Field Museum, so I went home and worked on doing some documentation. I updated the ReadMe file in &lt;a href=&quot;https://github.com/allisonchen23/ml_classifications&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;my Github for this machine learning project&lt;/a&gt;. Documentation is something I’ll really have to be on top of things about especially because not many people in this department are familiar with using python and machine learning.&lt;/p&gt;

&lt;p&gt;Although not terribly busy, today was a good day to prep me for the work I need to do next week!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 12, 2019 Today was essentially spent just making changes to the architecture/regularizers of my model and training it to hopefully get better results. The main issues I’m facing now is that validation accuracy fluctuates a lot. It’ll grow steadily for a little then drop down to almost 50% and grow a little more (such as in the image below).</summary></entry><entry><title type="html">Week 6: Continuing Machine Learning Project</title><link href="http://localhost:4000/food_for_thought/FM-week-6/" rel="alternate" type="text/html" title="Week 6: Continuing Machine Learning Project" /><published>2019-08-05T08:25:40-05:00</published><updated>2019-08-05T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-6</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-6/">&lt;h3 id=&quot;monday-august-5-2019&quot;&gt;Monday August 5, 2019&lt;/h3&gt;

&lt;p&gt;I took last week off for the Primers Program at PPG in Pittsburgh (wow that was a lot of P’s) and I absolutely loved it! From Sunday to Thursday, we just learned about the details of the company, what they do (which is mostly make paint and coatings), and about the culture they create. Something I really liked was the emphasis on environmental impact as well as the well-being of its employees. No matter what your position, the company wants you to keep growing and learning and that’s something I can stand by.&lt;/p&gt;

&lt;p&gt;Alas, the vacation came to an end, and today I was back to work! Like I mentioned last week, we have a model working, but I want to continue making it more similar to the Smithsonian model and improve its accuracy. Today, I mainly worked on randomizing the training, validation, and test data (although I’m not sure how big of a difference it makes). Previously, I had 1600 images for training and validation, which between each other would get randomized, but the test data were always the same 200 images. This also required separate folders for the test data. Today, I pooled all the images I wanted to use into one folder and had the code split up the images. One thing I did run into at the end of the day, however, was when I went to use the model to predict the family, it threw an error. The format of the data is still a numpy array, but for some reason, the function isn’t accepting it. I used the debugger to compare what I previously had to what I currently have, and the only difference was that the numpy array was an element in a Python list. For tomorrow, I’ll try just putting the numpy array in a list and see if it works.&lt;/p&gt;

&lt;p&gt;Additionally, I attended presentations from the Women in Science interns at the Field Museum today! There were 5 groups of 2 working in various departments and it was really cool to see the hands-on science that they were doing! Very inspiring to see my peers work on such cool projects :)&lt;/p&gt;

&lt;p&gt;Just getting back into the swing of things!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-6-2019&quot;&gt;Tuesday August 6, 2019&lt;/h3&gt;

&lt;p&gt;This morning I came in and fixed my testing-the-model function. The numpy array I wanted to use had the dimensions 256 x 256 x 3, but the older version (which was working) uses an array with the dimensions 1 x 256 x 256 x 3. To fix it, I simply reshaped the numpy array to the desired dimensions. It’s crazy how such a simple fix like this could take so long to figure out! Afterwards, I worked on preventing overfitting with my model. One thing that I implemented today was L2 Regularization which essentially uses the squares of the coefficients as penalties in the loss function, emphasizing the impact of large coefficients in the model. So if the model consists of many large coefficients, they will quickly negatively impact the loss function. This prevents the loss function from decreasing too quickly and helps with overfitting of the model.&lt;/p&gt;

&lt;p&gt;In the Smithsonian model, they used L2, but it was applied to the entire model in one step. However, with Tensorflow, we only apply regularizers layer by layer. After looking at some examples online, it seems that most people apply them to the Dense layers, so I did as well, starting with an alpha value of 0.01. (Larger alpha values result in a larger loss). Usually the alpha value is between 0 and 1, but the Smithsonian model uses 5 which struck me as odd. That’s definitely something I would like to ask our NEIU grad student, Beth, with whom I’m meeting tomorrow.&lt;/p&gt;

&lt;p&gt;To recap, my model currently takes in all the pictures (for training, validation, and testing) in 2 folders (one for Lycopodiaceae and one for Selaginellaceae) and divides them accordingly. This way, we aren’t always testing on the same images and it gets randomized each time we re-train the model. However, sometimes we also want to test on images that aren’t originally input; in those cases, I have two scripts that can test the model: one takes from 2 folders on the computer and the other uses the images originally put in with the training and validation images.&lt;/p&gt;

&lt;p&gt;I feel like the model currently is pretty good, but my next steps, I would like to increase the number of epochs and maybe the number of images used to train the data in order to increase accuracy in validation and testing data. Honestly, I’m not really sure when you’re “done” training a model. On a last note, this Friday marks the end of the high school Digital Learning Internship that we’ve been helping out with and I’m going to do a quick presentation on this project! I’m excited to share the work I’ve done, even though it’s not super ground breaking or anything in the field of CS, and see the presentations from other interns as well!&lt;/p&gt;

&lt;p&gt;Oh also, this afternoon, we went for some dim sum lunch at a place in Old Chinatown called Triple Crown and it was &lt;span class=&quot;standOut&quot;&gt;AMAZING&lt;/span&gt;. 10/10 recommend! Then we had some time just to relax, walk around, and check out the Chinatown shops! What a realxing afternoon! :D&lt;/p&gt;

&lt;p&gt;Have a great day!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-7-2019&quot;&gt;Wednesday August 7, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I continued making changes to the model and seeing the result. Right before I left for my meeting at NEIU, I trained the model with more images (around 1600 training images) and tested. At first, with 180 test images, it had around a 97% success rate, but when I introduced 500 new images, it plummeted to around 50% success rate. I didn’t have time to examine why this happened, but I will tomorrow! If I can’t find a good reason why this is happening, I will go back to using less images and see if the model “reverts”.&lt;/p&gt;

&lt;p&gt;At my meeting with Beth, I mostly recapped what I had done and we talked about some steps moving forwards, which are listed below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the same images for training/validation because we want to see the performance of the model, we don’t want the model to be affected by variables like randomization of the images used between training/validation and testing.&lt;/li&gt;
  &lt;li&gt;Test that the models I’m saving still work. I copied the .model files into a different folder, but I didn’t take the h5 files. I figured since the testing files only called upon CNN.model, it should be fine, but I would like to test this.&lt;/li&gt;
  &lt;li&gt;Keep trying to improve model! Once we get something satisfactory, we can test on larger sets of images and change images used.&lt;/li&gt;
  &lt;li&gt;Look into using metadata to help with the classification (Ask Matt if this is something worth doing). Maybe knowing something like location of collection could help the model predict the specimen. However, this will take a bit more research!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lots to work on, so I’ll definitely be busy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-8-2019&quot;&gt;Thursday August 8, 2019&lt;/h3&gt;

&lt;p&gt;Okay not going to lie, this week has been a lot of fun! Yesterday was one of the interns, Amelia’s, birthday but she wasn’t here, so today, we went to Jewel Osco to get her a cake and celebrated! But for my project, when I used the model we trained for new images, it was the same probability as a coin toss. My hypothesis is as follows:&lt;/p&gt;

&lt;p&gt;When selecting the training images, I took the first say 1000 images from each folder. Then I tested the model on the next maybe 100 images after that. These images were probably kind of similar since they were just taken in order from the online collection which means they may be grouped by time they were collected or even location and collector. This could mean that the images at the beginning of each folder may be drastically different than the images from the end of the folder. So, when I went to go test the model on new images, it would result in accuracy of around 50%. Which is &lt;span class=&quot;standOut&quot;&gt;very bad&lt;/span&gt;. So my next step here is to work on a helper function that will take &lt;i&gt;all&lt;/i&gt; the specimen we have and choose images periodically to spread out the sample.&lt;/p&gt;

&lt;p&gt;After implementing this technique of choosing training images, I tested it on images that were chosen with a similar technique. It seems results are more consistent which is good, but of course our accuracy took a hit. Now I want to work on building it back up to what we had with the misleading accuracy. I also want to consider testing on randomly selected test images (that of course, were not seen during training). I honestly don’t think most of this is too hard, but the sheer amount of images that I’m dealing with is a little intimidating and keeping eveerything straight (which files are correct, separating old batches from new) is getting a little overwhelming. I might take a bit tomorrow to organize better. Also! Tomorrow is the high school interns’ last day :( They’ll be presenting their projects and I’m excited to hear about them!&lt;/p&gt;

&lt;p&gt;Let’s end this week strong!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-9-2019&quot;&gt;Friday August 9, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I really cracked down, working on my machine learning project. Right now, I’m trying to combat the issue of sometimes when I used the model on test data, it would only have 50% ish accuracy despite validation accuracy being really high. This is because the model is overfitting, so I’m trying to implement different methods to fight this. First, I implemented some early stopping. This will stop the model if the validation loss starts to increase again, although it’s not ideal. I need to first change some things about my model to fight the overfitting before just stopping training. Currently, validation accuracy will remain at about 50% while training accuracy increases then suddenly validation accuracy will increase and fluctuate a bit. My next steps will be to implement more dropouts in my model, but less percent of the model at a time. If that doesn’t really work, I may look into some image augmentation, which seems to be a popular suggestion online. (Sorry for the short entry today!)&lt;/p&gt;

&lt;p&gt;Enjoy the weekend!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 5, 2019</summary></entry><entry><title type="html">Week 5: Machine Learning Ramps Up</title><link href="http://localhost:4000/food_for_thought/FM-week-5/" rel="alternate" type="text/html" title="Week 5: Machine Learning Ramps Up" /><published>2019-07-22T08:25:40-05:00</published><updated>2019-07-22T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-5</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-5/">&lt;h3 id=&quot;monday-july-22-2019&quot;&gt;Monday July 22, 2019&lt;/h3&gt;

&lt;p&gt;Hi everyone! I can’t believe we’re already nearing the end of July! Is it just me or is this summer seriously flying by?? Anyways, today was major progress on the third part of my project: classification using machine learning! So just a quick recap, there’s an article by the Smithsonian explaining how they were able to use a ML algorithm to sort two plant families that are morphologically similar &lt;span class=&quot;standOut&quot;&gt;(Lycopodiaceae and Selaginellaceae)&lt;/span&gt;. Check it out &lt;a href=&quot;https://bdj.pensoft.net/article/21139/element/5/3811021/&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;here&lt;/a&gt;! They also used an algorithm to find stained specimen sheets, but for now, we’re only focused on the family part. My current goal is to understand machine learning enough to essentially replicate what they did in the article. Then after that, we would like to apply a similar concepts to other species or categories. For example, with the staining example done in the article, there’s a really practical purpose to conduct that. Essentially, in the past, specimen were treated with mercury to preserve them longer, but over time, we’ve learned that this is actually bad for the specimen and it causes staining on the specimen sheets. Eventually, we’d like to find the specimen that have been treated and are now stained so we can remount them, allowing them to last longer.&lt;/p&gt;

&lt;p&gt;Last week, I created scripts to do a lot of the initial image processing so all the pictures are the same size (256 pixels by 256 pixels) and have the same resolution (96 ppi). Today, I started creating my own ML model using tensorflow and keras to train a model, following the tutorial &lt;a href=&quot;https://towardsdatascience.com/all-the-steps-to-build-your-first-image-classifier-with-code-cf244b015799&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;here&lt;/a&gt; (bless the people who make easy to follow tutorials like this one!) Before I started attempting to recreate the model from the Smithsonian paper, I just wanted to get something working with the model in the tutorial and I did! I only used 100 images from each family and 10 epochs (epoch = essentially a cycle) and the accuracy is pretty trash, as you can see below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/initial_model_accuracy.png&quot; alt=&quot;Graph of accuracy of training model from tutorial. It's pretty trash&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that I have that, I plan to follow these steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Change the layers to recreate the model from the Smithsonian Paper&lt;/li&gt;
  &lt;li&gt;Increase my training size&lt;/li&gt;
  &lt;li&gt;Test the model on a test batch!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Starting on step 1, I was almost able to finish replicating the paper’s model in Python. Although the Smithsonian used Mathematica, with the help of Google (honestly, I don’t know what I’d do on this project without Google) I think I’m able to figure out what the paper’s layers are doing then finding the relative equivalent in Python. I’ve almost finished that and hope tomorrow I’ll be able to get a small batch tested with hopefully higher accuracy than the random one I used today. Something I would like to investigate further is how do we know how many layers or filters to use? What filter size should we use for the convolution layer? How did they figure out what combination of layers worked best? It seems that creating a model is not that difficult, it’s the &lt;i&gt;why&lt;/i&gt; you choose certain characteristics of the model that I’d like to further explore.&lt;/p&gt;

&lt;p&gt;Well today was a very satisfying day and tomorrow we have our Python workshop! Can’t wait :D&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-july-23-2019&quot;&gt;Tuesday July 23, 2019&lt;/h3&gt;

&lt;p&gt;Hello! First thing I would like to talk about today is that I co-hosted my first Python workshop! It was for the other interns in the botany department as well as paleobotany and some high school interns. I think as a self reflection, overall it went pretty well! I think there was just a lot of information at once, especially for people who never used Python before. So in the future, if I were to do it again, I would be more careful about tailoring the workshop to the audience because of course everyone is at a different stage of learning! I sent out a feedback form, so hopefully that will help me become a better teacher in the future!&lt;/p&gt;

&lt;p&gt;More related to my project, I think I got a model that’s pretty similar to the one described in the Smithsonian paper. At least by what the layers are called/what I think they do. The difficult part in this was because the paper was done in Mathematica while I’m using TensorFlow in Python, so the types of layers/how to use them don’t carry over exactly. I’ve been doing a lot of research into what certain layers are intended to do in Mathematica and trying to find the Keras/TensorFlow equivalent for Python.&lt;/p&gt;

&lt;p&gt;With my first test, I used 100 images from each family (Lycopodiaceae and Selaginellaceae) to train/validate the model (10% of this batch are used for validation). The results of the accuracy are below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/training_100.png&quot; alt=&quot;Graphs of accuracy over epochs for model with training sample of 200 total images&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although the training accuracy increased over time, we see that by the second epoch, the validation accuracy plateaued, which is not a good sign. Additionally, when tested on the same test data as above, all the images were labeled as Selaginellaceae, leading me to believe overfitting is happening, but I can look into this more tomorrow.&lt;/p&gt;

&lt;p&gt;Exciting things are happening and I’m definitely kept busy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-july-24-2019&quot;&gt;Wednesday July 24, 2019&lt;/h3&gt;

&lt;p&gt;Today I took a bit of a break from the machine learning classification project. In the morning, a teacher from Northside College Prep came in and worked with us to create our own ozone detection strips! These strips can be used as indicators to see whether there are high levels of ozone in our area or not! They’re actually really easy to make; it was essentially mixing some cornstarch and water, add some sodium iodide, and painting them onto strips. Then we have to let them sit overnight.&lt;/p&gt;

&lt;p&gt;The majority of the afternoon was spent helping out some scientists and the high school Digital Learning Interns, but I did get to work on the CNN model a little bit. To combat the issues I faced yesterday, I decreased the learning rate of the model from the standard for Adam (0.001) to 0.0001 and that improved our accuracy for training and validation (see below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/decreased_lr.png&quot; alt=&quot;Graph of accuracy over epochs with learning rate at 0.0001&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks nice, but I believe the model is still overfitting to the training data. This hypothesis was supported when I tested the model on some test data with 10 images from each family and all of them were labeled as Lycopodiaceae (the opposite from last time). Online says this may be due to imbalance of input data in training, but I don’t think that’s the case because we have 800 of each family. Maybe not enough data? I still believe overfitting is an issue here, so we need to see how we can reduce that. It may be time to consult some experts.&lt;/p&gt;

&lt;p&gt;Although they’re tough, roadblocks are good! This means we’re learning :)&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-july-25-2019&quot;&gt;Thursday July 25, 2019&lt;/h3&gt;

&lt;p&gt;Happy Thursday! In the morning, we did a bit work with the ozone strips we prepped yesterday. We took them outside for a bit and tomorrow morning we can scan them to see the ozone levels found. It was nice to just walk outside around the museum and appreciate the gardens and all, like this nice picture below!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/flower.jpg&quot; alt=&quot;Nice flower by museum&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the machine learning project, I noticed that overfitting was still occurring with our training model. Looking online, I found that Regularization could help. I did see &lt;span class=&quot;standOut&quot;&gt;L2 Regularization&lt;/span&gt; in the Smithsonian paper, but didn’t quite understand what was happening and didn’t implement it yet. The Smithsonian paper set the L2 Regularization to 5 which I found very odd because doing some research online, it seemed that regularization values were generally very small, less than 1. Additionally, in Mathematica, L2 Regularization is something set for the model as a whole, but in Keras, it’s done layer by layer. Keras also gives options for regularization in bias, activity, and kernel, so for now, I just created a new layer after the first Dense layer and set all the regularizers to 0.01.&lt;/p&gt;

&lt;p&gt;There is no better way to describe my experience right now than research, guess, and check.&lt;/p&gt;

&lt;p&gt;Additionally, I found you can add custom functions which would help match my dropout layer to the Smithsonian one. I found that the Mathematica dropout layer not only drops half the values, but also multiplies the remaining ones by 2, but dropout in Keras only drops a certain percent of the values. Creating a custom function, I used the basis of the relu activation function with two changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I made the slope for negative values still 1, essentially making it an identity function&lt;/li&gt;
  &lt;li&gt;I multiplied the function by 2&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The model is running and saves, but when we want to load the model to test it, it’s unable to recognize my custom function which is something I’m still trying to figure out. In the end, I eventually took out the custom part because at this point, I’m not even able to test the model. Also, at the end of the day, I was training the model, but unfortunately left in a rush to catch my train. I’m not sure if this is normal, but I noticed that training the model takes up a LARGE majority of the computer’s memory and CPU, so that’s something I plan to look into tomorrow. I think before I continue tweaking and testing, it will be a good idea to update everyone from NEIU with what I’ve done (which may end up being completely useless) and just trying to learn more instead of just trying random things I find because that method is proving to be expensive (in matters of time)&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-july-26-2019&quot;&gt;Friday July 26, 2019&lt;/h3&gt;

&lt;p&gt;First, I’d just like to say we need to make more of an effort as a community to create a better world. To use less resources, less energy, and think every day how wecan make a positive environmental impact on this world. It’s all about give and take, not just always taking. I for one am going to look into making even just small daily habit changes that can add up to make a much larger impact. Anyways, regarding my machine learning project, today I made a break through! I was struggling for the last few days because although the training and validation accuracies would be okay, when it came to the testing data, basically the model just associated one class with all the data, even when I shuffled the data before testing. Well, after spending hours and hours researching, today I finally realized why, and it’s actually quite a dumb reason, so drumroll please!&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;It’s because I didn’t scale the RGB values of the pixels in the testing data!!! Basically, for the training model, we scaled all the RGB values for each pixel (usually between 0 and 255) to be between 0 and 1, but I forgot to do the same for the training data! So &lt;i&gt;of course&lt;/i&gt; we wouldn’t be getting correct predictions!&lt;/p&gt;

&lt;p&gt;After making that change, things began working again and I was actually able to make predictions! Initially, without making changes to the initial model, I was able to get about 93% accuracy with my test data, and now I’m working on changing my model to have it more closely match the Smithsonian paper and seeing the effects. I started to see a little overfitting, so I’ve been increasing drop out and decreasing learning rate, for example, to combat that. I’ve also been keeping a &lt;a href=&quot;https://docs.google.com/spreadsheets/d/15972K_rdv3zpnvnV--wSaTpiK0jnVDkBzhUWjre5BLg/edit?usp=sharing&amp;quot;&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;record of my changes and results&lt;/a&gt;. Check it out for an always up to date record!&lt;/p&gt;

&lt;p&gt;I won’t be in at all next week because I’m going to Pittsburgh, but this has been a great place to stop for myself!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday July 22, 2019</summary></entry><entry><title type="html">Week 4: Microplants Never Ends, Prep for ML Classification</title><link href="http://localhost:4000/food_for_thought/FM-week-4/" rel="alternate" type="text/html" title="Week 4: Microplants Never Ends, Prep for ML Classification" /><published>2019-07-15T08:25:40-05:00</published><updated>2019-07-15T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-4</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-4/">&lt;h3 id=&quot;monday-july-15-2019&quot;&gt;Monday July 15, 2019&lt;/h3&gt;

&lt;p&gt;Happy Monday! Right now in Microplants, we are working on matching the species to each measurement data entry that’s exported. And if you wanna know about the mess it is, stay tuned because I’m about to explain it. Basically, from our raw data that we’ve been cleaning all this time, there’s a column called ‘subject_ids’ that will end up being very important to us. See image (1) below. Then there’s another sheet that we call ‘MatchingIDs’ that has columns of subject_ids along with the name of the image used (see image (2)). So each time someone used the kiosk, maybe they got the same image as someone else, but a new subject_id was created. Then there’s &lt;i&gt;another&lt;/i&gt; CSV, which we call the voucher, that contains a whoooleee bunch of data, but the important stuff is two things (see image (3)):&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The species&lt;/li&gt;
  &lt;li&gt;Usually a substring of an image name (hopefully)&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/subject_ids.png&quot; class=&quot;images full&quot; alt=&quot;Showing subject_ids column in raw measurement data&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/matching_ids.png&quot; class=&quot;images full&quot; alt=&quot;Showing MatchingIDs CSV with subjec_id and image name&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/voucher.png&quot; id=&quot;voucher&quot; class=&quot;images full&quot; alt=&quot;Showing the voucher CSV&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;It’s not a very organized system but we make do with what we can. So last week and today, Jose worked on a script that used the voucher and the MatchingIDs CSVs (images (2) and (3)) to export a new sheet that matched the subject_id with a species using the image name and the substring of the image name. Then, we’re turning that exported CSV into a hash table (aka dictionary in Python) and using that along with the subject_ids of our original measurement data to have our final exported CSV contain a column for the species as well. We’re using a hash table because we want quick look up and we don’t need to worry about putting any of the elements in order.&lt;/p&gt;

&lt;p&gt;I’m currently working on integrating his script into our main code and exporting the final document. Wish me luck that I don’t accidentally ruin everything :)&lt;/p&gt;

&lt;p&gt;Stay tuned for more details!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-july-16-2019&quot;&gt;Tuesday July 16, 2019&lt;/h3&gt;

&lt;p&gt;Happy Tuesday! This morning was spent working on some Excel “algorithms” and learning to use custom functions. Basically, the museum is trying to compile a list and summary of all the education outreach programs that they do and compiling the data entried to be presented in a useful manner. I don’t think it was too difficult for me to figure out how to do, but I did have to implement a custom script and it took me a little bit of time to look up the functions and integrate them into my code. I also learned that when you use custom functions to refer to cells outside the current sheet, when those values change, the function doesn’t get automatically updated. Instead, I had to copy the formula, delete it, and paste it in again. Tomorrow, I’ll work on creating a button or feature that will automatically refresh those specific values. On another note, the Python workshop I’m co-hosting with Jose will officially be held next Tuesday at 1pm! I’m excited to teach Python to the other interns and staff and hopefully get them excited about coding!&lt;/p&gt;

&lt;p&gt;Going back to my projects, for Microplants, currently we had a few different functions that outputted CSV files that our main code then took in, so I spent some time eliminating the need for that extra output and directly having our main function use these helper functions. There are actually quite a few steps to this, but I compared the final output with the output we had before and they’re the same as expected, so we know that I integrated these files correctly! We also realized that we need to jump onto the machine learning with fern classifications a little bit more, and our first step is to write a script to download images of the ferns from the online database for classifications. There isn’t an option to download the images directly from the database, but you can download a CSV with the specimen IDs and the image URL. Using the handy-dandy Google again, I created a script that can take this CSV, download the images into a folder, and name them based on their ID. I didn’t try it on the actual images we want because there was a pretty large amount, but I tried it on some test cases and it seems to work! Also, I didn’t want to take up storage on my computer with all those fern images.&lt;/p&gt;

&lt;p&gt;During this extremely hot part of the summer, stay cool!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-july-17-2019&quot;&gt;Wednesday July 17, 2019&lt;/h3&gt;

&lt;p&gt;Today, a lot of the interns went to Lincoln Park Conservatory for a field trip, so it’s been a pretty quiet day. In the morning, I worked on my image download file to allow people to run it in the terminal while allowing for arguments. Currently, I’m allowing them to input the CSV file they want images downloaded from as well as the location of the image downloads. I looked into this earlier, but since I kept forgetting the “syntax” of calling the function with arguments, I’m including a picture below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/calling_w_argparse.png&quot; alt=&quot;terminal syntax to call function with argparse parameters&quot; class=&quot;images full&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Regarding the machine learning part of the project, I mentioned yesterday that I created a script to download the specimen images from an online database. Today, I worked on setting up Python and my script on the desktop computers at the museum because I was not about to download around 7000 images onto my personal computer. It took a while mostly because we needed admin permission, so I didn’t get to test yet, but tomorrow will be my day! The CSV file where I’m pulling hte imge URLs from has a column called ‘coreid’ that I was going to use to identify each picture with, but them I found that the coreid is a little bit arbitrary. Instead, each image also has a barcode that’s part of the museum’s specimen database that I would like to use instead. The only challenge is that it’s on yet another CSV that I can download and just need to link into my script. This additional CSV has both the coreid and the barcode number which I can create into a hash table (aka dictionary in Python) to so it won’t kill the programs efficiency.&lt;/p&gt;

&lt;p&gt;Which is another thing; if I have more time, I would like to look into improving the efficiency of the code; currently I feel it’s quite slow, but I’m not confident about a method of improving it right now. I can research that tomorrow as well.&lt;/p&gt;

&lt;p&gt;Thanks for reading and take it easy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-july-18-2019&quot;&gt;Thursday July 18, 2019&lt;/h3&gt;

&lt;p&gt;Today was mainly spent dealing with image pre-processing for the fern machine learning part of the project. First, we can’t directly batch download all the images from the database we’re using, so I polished up a script that will take the links and download them. The only thing is that they don’t all download with the same resolution  or size, but this is to be explored further. When writing ths script, I mainly used a column labeled ‘identifier’ that contained a link to the image. But then I found that some of the images didn’t have an identifier link, but they did have a link under a column called ‘goodQualityAccessURI’. For some reason, however, downloading from ‘goodQualityAccessURI’ takes longer than downloading from ‘identifier’, so we only download from there if we’re unable to find a link in identifier. I would like to look into this ore because it could be one of the reasons we don’t have uniform image sizes. The next step would be to resize all the images to the same resolution and size. My other issue I ran into with this is I don’t know a good desireable size to use for these images. Doing some research, it looked like most ML algorithms use square images of at most around 128 px by 128 px, but currently our images are more like 1500 px by 2000 px, which is significantly larger. I’m afraid that compressing down to that size would possibly lose some of the details of the image, but we can explore this further. I guess the first step for tomorrow will be using Python or GIMP to just get them to the same size and resolution first.&lt;/p&gt;

&lt;p&gt;At a bit of a block, but we’ll get through it!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-july-19-2019&quot;&gt;Friday July 19, 2019&lt;/h3&gt;

&lt;p&gt;This Friday was really quiet at the museum. Most of our intern take the day off, so today, it was just 2 of us: Jessica and me. I was kinda struggling and a bit stressed with how I left things with the image proprocessing yesterday, but luckily, in the morning Matt came up pretty early and we talked a few things out. For example, for some reason, not all the images downloadedfrom the Pteridophyte Portal database were the same size or resolution and I was nervous that it would cause some issues in resizing them. Looking at &lt;span class=&quot;standOut&quot;&gt;the paper from the Smithsonian&lt;/span&gt;, we decided to resize all the images to 256px by 256px and not to worry about image resolution at this point. We know for a fact that you can use Photoshop to batch resize these images, but I wanted to try to use Python to do so instead. I definitely know it’s possible, I just wasn’t sure how to do it. The plan was to resize them using Python and then resize a few manually with GIMP and see if there’s a big difference.&lt;/p&gt;

&lt;p&gt;Using my resources (aka the infamous Google), I found that Pillow is a relatively common Python package for image processing and initially, I used that. Just looking at the resulting images, they looked extremely distorted and any text was 300% impossible to read. I knew that by reducing the image size, we would lose a lot of clarity, but this seemed more distorted than expected. After resizing in GIMP, I could clearly see the GIMP software did a much better job of preserving clarity. I was about to just explore using GIMP to process all of the images, but then I remembered another image processing software,&lt;span class=&quot;standOut&quot;&gt; OpenCV&lt;/span&gt;, that I had heard about. I had to download packages for that (just Google ‘OpenCV Python’) and resized the image which produced a &lt;i&gt;much&lt;/i&gt; clearer image. Check out the comparisons below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/sela_5887.jpg&quot; class=&quot;images third&quot; alt=&quot;Example of original uncompressed image&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/sela_5887_pillow.jpg&quot; class=&quot;images full&quot; alt=&quot;Resized image using Pillow&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/sela_5887_gimp.jpg&quot; class=&quot;images full&quot; alt=&quot;Resized image using GIMP&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/sela_5887_openCV.jpg&quot; class=&quot;images full&quot; alt=&quot;Resized image using OpenCV&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p class=&quot;caption&quot;&gt;The top center image is the original image. On the second row from left to right is the resized image using Pillow, GIMP, and OpenCV. Because the images are small on this site, the difference is not immediately apparent, but look in the upper and lower right corners at the label to see a difference.&lt;/p&gt;

&lt;p&gt;After testing this function on an image, I added to my code so the program takes in a folder and will resize all the images in the folder. After testing that on a small subset with original images of all the same size, I used it with all the images that were exported with a width of aroudn 1500 pixels and a resolution of 72 ppi. Once that worked, just for funsies, I tested it on all the images, and to my knowledge, they all exported with the same size and resolution of 96 ppi (that’s just what OpenCV gave me). So yay! Now we have scripts to not only download the images, but also to resize them appropriately :D&lt;/p&gt;

&lt;p&gt;Looking into image classification models, I found it to be necessary to have a CSV with all the image names and labels (in our case, family of the specimen) to train the model. This is something I forgot initially, so I also added that to my script before calling it a day!&lt;/p&gt;

&lt;p&gt;Whew! It’s been an exciting week preparing for the machine learning model, but I hope everyone has a great weekend and stay cool in this hot and humid Chicago summer!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday July 15, 2019</summary></entry><entry><title type="html">Week 3: Statistical Analysis for Microplants, Image Editing, and Basic Machine Learning</title><link href="http://localhost:4000/food_for_thought/FM-week-3/" rel="alternate" type="text/html" title="Week 3: Statistical Analysis for Microplants, Image Editing, and Basic Machine Learning" /><published>2019-07-08T08:25:40-05:00</published><updated>2019-07-08T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-3</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-3/">&lt;h3 id=&quot;monday-july-8-2019&quot;&gt;Monday July 8, 2019&lt;/h3&gt;

&lt;p&gt;Hope everyone had a great July 4th weekend! Today, we were back on the grind, starting with analysis of the Microplants data that we previously cleaned. To jog your memory, recall that the purpose of us cleaning Microplants kiosk data and combining it with age data is to see if there is a difference in the accuracy of measurements (determined by the angle between the major and minor axis) between the different age groups. Today, I worked on learning how to use R and without even doing any statistical tests and just graphing the data, we can see that there in fact is a difference between groups, as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/age_accuracy.PNG&quot; alt=&quot;Left is the counts of good and bad data by age group and right side has the percentage of good and bad data within the age group&quot; class=&quot;images three_fourths&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next task here is performing a linear regression model and chi square test to have a statistical measurement of how different the results are depending on age group and how much more likely one age group is to perform more accurate measurements than another age group.&lt;/p&gt;

&lt;p&gt;Additionally, today I played around with GIMP, an image editing software that is used to binarize the images of specimen for MorphoSnake to read in. I was able to eliminate the background of an image, but I’m still working on getting only the desired parts of the specimen black. The tricky part here is that we don’t want the lobules (leaf looking like parts) to overlap, since that could mess up the morphosnake measurements. The good news is that in most of the images you can see both dorsal and ventral lobules (where ventral lobules show the front and dorsal show the back) The ventral lobules tend to be more “skeletal” and if we can just keep those, that’s sufficient. Currently, we’re working on creating a script that will be able to recognize which pixels are part of the stem and ventral lobules and be able to discard the dorsal parts, but it’s very much in the process.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-july-9-2019&quot;&gt;Tuesday July 9, 2019&lt;/h3&gt;

&lt;p&gt;This morning was spent working on the statistical analysis of our Microplants data in R. We’re performing a binomial test on the data (since our results have two options: good or bad) and initially I was facing issues with the data structures in R. Then I learned that matrices in R are homogenous, meaning they can only hold data of one type, while data frames are heterogeneous, so they can have both strings and integers. After implementing those changes, we were able to get the glm (general linear model) function working with our data and we found that the data &lt;i&gt;is&lt;/i&gt; statistically significant, since the probability (p-values) were incredibly low. Additionally, we calculated the odds ratio which help us determine just how much more likely one group is to create a good measurement than another.&lt;/p&gt;

&lt;p&gt;After working on the statistics side in the morning, we had a team meeting with the rest of botany, the paleobotany department, and the Digital Learning Internship students. We did another round of introductions, which is helping me learn everyone’s names! Additionally, Jose and I explained our projects and did a quick demo of MorphoSnake to show them how the software worked. Our explanation was pretty quick, but I hope it made sense!&lt;/p&gt;

&lt;p&gt;After lunch, I found this plug-in for GIMP (the image processing software) that allows you to write python scripts using GIMP, but I’m struggling to get it working on Ubuntu. First, there aren’t as many resources out there regarding this area and a lot of the tutorials assume things that aren’t there in our version of Ubuntu and GIMP, so a lot of it is trying new things and working from there. At the same time Jose is working on just using Python packages to do the image editing and hopefully between the two of us exploring different paths, we’ll be able to get something going soon.&lt;/p&gt;

&lt;p&gt;Today was tiring but pretty rewarding, get some rest!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-july-10-2019&quot;&gt;Wednesday July 10, 2019&lt;/h3&gt;

&lt;p&gt;Okay not gonna lie today was not super productive, but it’s okay, we all have those days. In the morning, I went to the intern breakfast downstairs and got to meet some of the other interns at the museum! We also had a Q&amp;amp;A with the president of the museum and it was really cool to see this more personable side and actually kind of get to know him. After that, I was working more with GIMP and MorphoSnake, trying to get an image of a plant skeletonized. I was able to take the following image and kind of skeletonize it (Not sure if I could &lt;i&gt;actually&lt;/i&gt; say I was successful) I ended up making it black and white, intensifying the contrast, using the smart scissors tool to select more of the stems, and eventually using the paint tool to draw out the stem. However, when I tried using it in MorphoSnake, it threw an exception. Still trying to figure out why it wasn’t accepting this image, but in the mean time, I started working on other things.&lt;/p&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;column_2&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/rostrata.jpg&quot; class=&quot;images full&quot; alt=&quot;unedited image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_2&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/rostrata_skel.png&quot; class=&quot;images half&quot; alt=&quot;skeletonized image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;p class=&quot;caption&quot;&gt;The right is an unedited image taken under a microscope and on the right is a skeletonized version.&lt;/p&gt;

&lt;p&gt;After lunch, I started the TensorFlow Machine Learning package sent from Beth, a graduate student at Northeastern Illinois University. I basically followed a tutorial using keras and tensorflow to create an algorithm that matches pictures of articles of clothing to the correct name. It uses 60K training cases and 10K test cases. Although I don’t fully understand what each statement is doing, I understand the general gist of the program, and I think it’s a good place to start before I move onto more complex ML problems. Additionally, next week, Jose and I will be hosting a Python workshop, so I went through his notes and added some of my thoughts. I think learning computer science to teach others computer science is one of the most exciting parts of the field! It’s something that everyone wants to learn about these days, and I hope the other interns/people attending our quick class are excited to learn! Lastly, I started documenting and commenting in the Microplants cleaning script. I want someone to be able to look at the code and understand what every line does. Tomorrow, that’s something I’ll continue working on.&lt;/p&gt;

&lt;p&gt;Happy Hump Day!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-july-11-2019&quot;&gt;Thursday July 11, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I helped organize the herbarium, taking a break from all this computer stuff. Basically, the museum is in the process of digitizing the specimen collection, but there were a few fern specimen that have to be re-imaged or didn’t get imaged in the first place. Teaming up with some interns from the Digital Learning Internship, we looked for those sheets and organized them to be sent to the imaging lab. Although it’s not the most exciting work, it’s definitely necessary and it was nice to get to know some of the high school interns better!&lt;/p&gt;

&lt;p&gt;In the afternoon, I worked on MorphoSnake stuff again. Using the paint tool in GIMP, I took an image of a plant, traced what I thought would be the stems and threw that image into MorphoSnake. Something that I learned is somethimes, after editing the image, MorphoSnake can’t always accept PNGs if they have alpha layers or transparencies. So to be safe, I started using JPGs. Below, you can see the different stages of this process. The only issue is that drawing them by hand is not necessarily something the machine can learn to do easily. Thus, we may need to look into a different method of skeletonizing the images.&lt;/p&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/f_rostrata.png&quot; class=&quot;images full&quot; alt=&quot;unedited image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/f_rostrata_skel.png&quot; class=&quot;images full&quot; alt=&quot;skeletonized image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/f_rostrata_MS.png&quot; class=&quot;images full&quot; alt=&quot;skeletonized image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;caption&quot;&gt;Left: unedited image of frullania rostrata. Center: Skeletonized image of f. rostrata using paint tool on GIMP. Right: Skeletonized image used in MorphoSnake.&lt;/p&gt;

&lt;p&gt;Since the technique above may not be reasonable to get a computer to do, I explored altering the image as a whole. For example, to obtain the sequence of images below, I took the following steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use the fuzzy select tool and set the threshold relatively high (around 80) to select entirely the main object)&lt;/li&gt;
  &lt;li&gt;Go to Select -&amp;gt; Invert&lt;/li&gt;
  &lt;li&gt;Choose the fill color to be white and use the bucket fill option to make the background all white.&lt;/li&gt;
  &lt;li&gt;Go to Color -&amp;gt; Threshold and adjust the slider values until a somewhat desired image is produced. The histogram shows how many pixels of the image are in that range. Pixels inside the range are converted to white and everything else is black.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/f_rostrata.png&quot; id=&quot;f_rostrata&quot; class=&quot;images full&quot; alt=&quot;unedited image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/f_rostrata2_blob.png&quot; id=&quot;f_rostrata2_blob&quot; class=&quot;images full&quot; alt=&quot;skeletonized image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;column_3&quot;&gt;
        &lt;img src=&quot;/assets/images/blog/fm/f_rostrata2_MS.png&quot; class=&quot;images full&quot; alt=&quot;skeletonized image of f. rostrata&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;caption&quot;&gt;Left: unedited image of frullania rostrata. Center: Skeletonized image of f. rostrata using whole image editing on GIMP. Right: Skeletonized image used in MorphoSnake.&lt;/p&gt;

&lt;p&gt;I’m currently trying to look into other options for image editing. Victoria suggested looking into Google Vision API, but that seems to be more just for recognition of objects in the image. It may be a place to start, but I’m not sure if it can do the image editing for us. I’ll have to check in with Jose tomorrow and touch base on ideas.&lt;/p&gt;

&lt;p&gt;A bit of a block, but we’ll get through it!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-july-12-2019&quot;&gt;Friday July 12, 2019&lt;/h3&gt;

&lt;p&gt;he morning was spent organizing with the high school interns again! Additionally, I spent some time looking more into convolutional neural networks and reading up on them to understand a bit more. I also looked into gradient descent functions to minimize cost, since those are pretty commonly used. In the afternoon, I worked with Jose to add some finishing touches to the Python workshop that we’re hoping on hosting next Thursday for the other interns/employees in botany, paleobotany, and the high school interns. On a completely other note, the next step in the Microplants project will be to match each measurement entry with the correct species. We have a list of subject ID’s and their corresponding species from someone who worked on this a year ago, so our next step will be to incorporate this matching into the cleaning function. I didn’t think of this originally, but we could look into implementing a hash table for easy look up. It’s been a while since I worked with those, so I’ll have to look into the purposes and syntax in Python again.&lt;/p&gt;

&lt;p&gt;Short and sweet ending, enjoy your weekend!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday July 8, 2019</summary></entry></feed>