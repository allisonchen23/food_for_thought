<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/food_for_thought/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/food_for_thought/" rel="alternate" type="text/html" /><updated>2019-12-12T22:58:41-06:00</updated><id>http://localhost:4000/food_for_thought/feed.xml</id><title type="html">Thoughts for Food</title><subtitle>Welcome to my blog! I'll be posting basically random thoughts and updates on any current projects/work I'll be doing.</subtitle><entry><title type="html">Talk to Your Uber Driver</title><link href="http://localhost:4000/food_for_thought/talk-to-your-uber-driver/" rel="alternate" type="text/html" title="Talk to Your Uber Driver" /><published>2019-12-12T09:01:00-06:00</published><updated>2019-12-12T09:01:00-06:00</updated><id>http://localhost:4000/food_for_thought/talk_to_your_uber_driver</id><content type="html" xml:base="http://localhost:4000/food_for_thought/talk-to-your-uber-driver/">&lt;p&gt;Okay hello! It’s been a &lt;em&gt;really&lt;/em&gt; long time since I’ve updated this, so I think we can safely say it’s been a busy quarter. I finished my finals yesterday (woo! freedom!) and today as I was on my way to LAX to fly back to Chicago, I was inspired to write a blog post about this after having a wonderful conversation with my Lyft driver. I know the title of this post is ‘Talk to Your Uber Driver’, but ‘Talk to Your Ride Share Driver’ just didn’t have the same ring.&lt;/p&gt;

&lt;p&gt;Somehow at school, I’m known to be relatively outgoing, and I tend to find it less difficult to strike up a conversation in the dining halls or elevator. (I really don’t mean for this to sound like a flex or anything, I promise this is relevant), and when some people ask my how I developed this skill, my honest answer is &lt;strong&gt;talk to your Uber driver&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Usually this is responded with a light laughter then awkward silence when they realize I’m serious. Let’s think about it for a second! First, you’ll probably never see the driver again, so what’s the worst that could happen? You have an awkward 20 minute drive and then you both completely forget about each other and no harm no foul. But what’s the &lt;em&gt;best&lt;/em&gt; that could happen? You get offered a job (which actually has happened to a good friend of mine)! But all joking aside, the most common outcome is that you get to talk to someone completely new with a totally different perspective on life, and at the end of the drive you leave the car wondering how time flew by so quickly!&lt;/p&gt;

&lt;p&gt;However, I can’t take credit for this idea; it was actually suggested to me by one of my good friends Matthew Wang (check out &lt;em&gt;his&lt;/em&gt; website &lt;span class=&quot;standOut&quot;&gt; &lt;a href=&quot;https://matthewwang.me/&quot;&gt;here&lt;/a&gt; &lt;/span&gt;) who is always able to strike up a conversation with our Uber drivers and I’m happy I get to learn from him :D&lt;/p&gt;

&lt;p&gt;In my experience, there hasn’t ever been a time that I regretted talking to my Uber driver, but each time, I learn a unique story of how they got to where they are today! It’s really eye opening for me to talk to people from backgrounds that are really diffrent from mine. Personally, I think it helps me see that there’s not one correct path to live life and “be successful”. I’m meeting all these people who are pursuing their passions and finding fulfillment in their lives despite barriers. I truly believe everyone has a unique and amazing story that tells who they are and how they got to where they are today, and I really love to just sit and listen to my Uber drivers share their stories and passions with me.&lt;/p&gt;

&lt;p&gt;Especially because each and every Uber driver is different, striking up a conversation with each one helps me  learn how to connect with different people. As an extreme example, you probably don’t talk to you grandparents the same way you talk to your best friends, and although the differences are more nuanced from driver to driver, a similar concept applies. It’s about learning how to pick up different almost “talking strategies” that allow you to have engaging conversations with almost anyone. I’ll be honest though, most of the time it’s simply finding a topic that the other person is passionate about and from there, just let the conversation flow :)&lt;/p&gt;

&lt;p&gt;From my LA Uber/Lyft experiences, I’ve met some amazing and really memorable people, such as a guy who’s opening up his own Brazilian Jiu Jitsu studio, a comedian who’s performed for Navy Seals, and one guy in a VR startup who actually also got his current job from driving someone in his Uber. I’ve also had just some amazing conversations such as mine today where we talked about things like the development of social media and the oil/energy industry. It was amazing to me to be able to talk about such important topics with someone I had just met, and I definitely left feeling refreshed from hearing someone else’s opinion and thoughts.&lt;/p&gt;

&lt;p&gt;Now, I’m not saying to go and pour out your whole life story to the Uber driver (first, that would be weird and second, please think about your personal safety!) but it doesn’t hurt to try to strike up a conversation starting with some small talk. Remember, still be safe and avoid oversharing; if things start to take a turn south, please be careful and use your discretion on how to proceed.&lt;/p&gt;

&lt;p&gt;Of course, there are days where I’m just utterly exhausted and can’t keep up a conversation for my life, or sometimes the driver is just not reciprocating the conversation. That’s okay! Like I said in the beginning, you’ll get out of the car 20 minutes later and you and the driver will probably both forget each other. But tl;dr to this post, using Lyfts and Ubers as opportunities to learn about someone new and develop interpersonal skills is something that I have found personally rewarding and hope you all can give it a shot!&lt;/p&gt;

&lt;p&gt;Wishing you all the best and happy holidays!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Okay hello! It’s been a really long time since I’ve updated this, so I think we can safely say it’s been a busy quarter. I finished my finals yesterday (woo! freedom!) and today as I was on my way to LAX to fly back to Chicago, I was inspired to write a blog post about this after having a wonderful conversation with my Lyft driver. I know the title of this post is ‘Talk to Your Uber Driver’, but ‘Talk to Your Ride Share Driver’ just didn’t have the same ring.</summary></entry><entry><title type="html">Best of Both Worlds</title><link href="http://localhost:4000/food_for_thought/best_of_both_worlds/" rel="alternate" type="text/html" title="Best of Both Worlds" /><published>2019-10-10T10:01:00-05:00</published><updated>2019-10-10T10:01:00-05:00</updated><id>http://localhost:4000/food_for_thought/best_of_both_worlds</id><content type="html" xml:base="http://localhost:4000/food_for_thought/best_of_both_worlds/">&lt;p&gt;Something I realized this year and last year when coming to college, I feel like there’s such a divide between my UCLA life and home life. I’m not sure if it’s because the environment is just completely different or because I myself am a different person too.&lt;/p&gt;

&lt;p&gt;When I come to UCLA, I tend to lose contact with people from home, even my family, which I know is definitely not right. I want to keep in touch and all, but I tend to get so caught up in the moment with academics and living in the present that it tends to slip my mind.&lt;/p&gt;

&lt;p&gt;And then this summer, when I went home, I felt the exact opposite! I was able to reconnect with people from home, but I felt I could’ve been better with keeping in touch with my friends from school. Maybe I just need to live in the moment less and remember to keep the people I love and care about close to me, even if they’re a few hundred miles away.&lt;/p&gt;

&lt;p&gt;Can anyone else relate?&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Something I realized this year and last year when coming to college, I feel like there’s such a divide between my UCLA life and home life. I’m not sure if it’s because the environment is just completely different or because I myself am a different person too.</summary></entry><entry><title type="html">Understanding People (well, trying to)</title><link href="http://localhost:4000/food_for_thought/understanding_people/" rel="alternate" type="text/html" title="Understanding People (well, trying to)" /><published>2019-09-12T06:00:40-05:00</published><updated>2019-09-12T06:00:40-05:00</updated><id>http://localhost:4000/food_for_thought/understanding_people</id><content type="html" xml:base="http://localhost:4000/food_for_thought/understanding_people/">&lt;p&gt;This topic has been something that really interested me this summer. I talked to my mom a lot about it and from her, realized that working effectively with people is probably one of the hardest things we have to learn to do. It’s not something you can only read from a textbook and understand immediately; it takes many experiences and practice to learn how to work with people without offending or hurting anyone.&lt;/p&gt;

&lt;p&gt;For example, we’ve all been in a situation where a disagreement arises, but we &lt;em&gt;know&lt;/em&gt; we’re right. How can we lead the other person to reach our conclusion? From experience, you probably know that fighting and arguing usually leads to nowhere, but too often, we let our emotions get the best of us. We don’t always do what’s rational or expected, which is part of what makes us human! Going back to the question, however, how can we guide people to our point of view?&lt;/p&gt;

&lt;p&gt;Currently, I’m reading the infamous book &lt;em&gt;How to Win Friends and Influence People&lt;/em&gt; by Dale Carnegie (which was written back in 1936) and it brings up fair points on this exact issue. To be honest, I do feel the wording of the title could be changed, it already seems a little ingenuine! Anyways, Carnegie delineates that we of course want to avoid arguing at all costs. In the case there is a disagreement, however, he mentions to stay calm, listen to the other person, and ask questions (kind of like Socrates!). Additionally, it may help to start the conversation with a totally different topics, such as one the other person genuinely cares about, to “warm them up”.&lt;/p&gt;

&lt;p&gt;These techniques seem fine and dandy, but for me, they bring up the question of genuinity. Is it right to intentionally talk about something you &lt;strong&gt;know&lt;/strong&gt; this other person will like? To be the devil’s advocate here, it seems like the book is suggesting you bring up a topic that you may not be genuinely interested in, but you feign it to get that person to like you more and be more willing to listen to you. I definitely understand that if you can connect with someone personally, even if there is a disagreement, they will be more likely to listen to your ideas. However, is your intention with this simply to get what you want or to actually understand this other person and their interests? Perhaps I’m thinking too far into this.&lt;/p&gt;

&lt;p&gt;Another section of the book mentions how if you know you’re in the wrong, to bring it up before the other person does and in a way that they can feel like they’re showing you mercy. For example, if you get pulled over for speeding, instead of trying to fight the cop or giving excuses on why you were speeding, Carnegie suggests that if you admit to what you did and almost in a pleading manner, the police is more likely to be lenient. This is because of people’s desire to have their ego boosted. If you fought the cop on this, denying that you should get ticketed, the cop may feel like his/her authority is being tested and fight back. Everyone wants to feel important and by insisting to give you a ticket you don’t want, the cop can feel important. On the other hand, if you admit you’re wrong, the cop is unable to argue with that and in order to feel important, he/she may show mercy and let you go. (Obviously, this is not saying that if you get pulled over and try this tactic it will work, it’s just an example very similar to one Carnegie outlines in the book.)&lt;/p&gt;

&lt;p&gt;Again to me, it seems a little excessive and almost manipulative to intentionally grovel just because you think you can get out of a ticket or a consequence. I agree with admitting when you’re wrong, but I believe there are better ways to do it… This to me just seems like the other end of the spectrum than arguing incessantly and allowing your temper to take over, and I personally think I’d feel more comfortable with an approach in the middle: staying calm and admitting if you’re wrong but not to the point of groveling.&lt;/p&gt;

&lt;p&gt;For me in all of this, I think it goes back to the intentions. If your intentions are to avoid conflict, I think actions similar to the two outlined above may be okay. But if your intentions are solely to get exactly what you want and you implement the techniques above, that’s where the line blurs a little.  But then again, can you always tell what someone’s intentions are? I myself am still trying to develop my own stance on this topic.&lt;/p&gt;

&lt;p&gt;Anyways, I recommend this book! &lt;em&gt;How to Win Friend and Influence People&lt;/em&gt; by Dale Carnegie. I think there are insightful things and great personal examples in the book, but I also think simply following his steps are not enough and as a reader, you should think about each one pretty carefully. They’ve got to be internalized for each person and there has to be genuine concern and care in the actions (that’s the tough part!)&lt;/p&gt;

&lt;p&gt;Just some topics to reflect on:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How do you tend to resolve conflict and settle disagreements? Do they normally end positively?&lt;/li&gt;
  &lt;li&gt;How can you be genuine about your actions/intent in every day situations?&lt;/li&gt;
  &lt;li&gt;Do you have a technique that helps you work well with people? I’d love to hear about it!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Best wishes,&lt;/p&gt;

&lt;p&gt;Al&lt;/p&gt;</content><author><name></name></author><summary type="html">This topic has been something that really interested me this summer. I talked to my mom a lot about it and from her, realized that working effectively with people is probably one of the hardest things we have to learn to do. It’s not something you can only read from a textbook and understand immediately; it takes many experiences and practice to learn how to work with people without offending or hurting anyone.</summary></entry><entry><title type="html">Banana Bread with Homemade Oat Flour</title><link href="http://localhost:4000/food_for_thought/banana_bread/" rel="alternate" type="text/html" title="Banana Bread with Homemade Oat Flour" /><published>2019-09-10T08:25:40-05:00</published><updated>2019-09-10T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/banana_bread</id><content type="html" xml:base="http://localhost:4000/food_for_thought/banana_bread/">&lt;p&gt;Prep Time: 15 minutes&lt;/p&gt;

&lt;p&gt;Cook Time: 45 minutes - 1 hour&lt;/p&gt;

&lt;p&gt;Serves: a LOT (about 1 loaf of bread)&lt;/p&gt;

&lt;p&gt;Difficulty: Coconut Mall from Mario Kart&lt;/p&gt;

&lt;h3 id=&quot;ingredients&quot;&gt;Ingredients&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Remember, these are all relative ish! Adapt as you wish!&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1/2 cup extra virgin olive oil + extra for greasing the pan&lt;/li&gt;
  &lt;li&gt;2/3 cup &lt;strong&gt;loose&lt;/strong&gt; brown sugar&lt;/li&gt;
  &lt;li&gt;1 tsp vanilla&lt;/li&gt;
  &lt;li&gt;2 eggs&lt;/li&gt;
  &lt;li&gt;3 mashed ripe bananas&lt;/li&gt;
  &lt;li&gt;1/4 cup milk (or any milk substitute, or even water)&lt;/li&gt;
  &lt;li&gt;2.5 cups oats&lt;/li&gt;
  &lt;li&gt;1/2 tsp salt&lt;/li&gt;
  &lt;li&gt;1/2 tsp baking soda&lt;/li&gt;
  &lt;li&gt;Add ins; my favorites are walnut and dark chocolate chips! But any type of nut, dried fruit, shredded coconut, etc will be fantastic&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;instructions&quot;&gt;Instructions&lt;/h3&gt;

&lt;h4 id=&quot;making-oat-flour&quot;&gt;Making Oat Flour&lt;/h4&gt;

&lt;p&gt;As a general rule of thumb, oat flour can substitute regular flour! Just be careful because the oats tend to absorb liquid a lot quicker than all purpose flour, so some modifications may have to be made. If a recipe asks for 1 cup of all purpose flour, you can blend 1 1/4 cups of oats to make approximately 1 cup of oat flour.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pour your oats in a blender or food processor (I personally prefer a blender!)&lt;/li&gt;
  &lt;li&gt;Start on a low speed and gradually bring your blender to high speed&lt;/li&gt;
  &lt;li&gt;Blend/Process oats until it is a fine flour (approximately 30 seconds)&lt;/li&gt;
  &lt;li&gt;Shake it around a bit to make sure there are no large chunks of oats that got left behind&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;rest-of-instructions&quot;&gt;Rest of Instructions&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Preheat the oven to 350F&lt;/li&gt;
  &lt;li&gt;Using an electric beater (or whisk and a really strong arm), cream together the oil and brown sugar&lt;/li&gt;
  &lt;li&gt;Add vanilla and eggs and beat until consistent&lt;/li&gt;
  &lt;li&gt;In a separate bowl, combine the dry ingredients&lt;/li&gt;
  &lt;li&gt;Alternate adding the dry ingredients with adding the mashed bananas and milk into the liquids. You can use a rubber spatula to mix at this point&lt;/li&gt;
  &lt;li&gt;Grease your breadpan using just a little bit of olive oil (just enough to cover the inside surfaces)&lt;/li&gt;
  &lt;li&gt;Pour your batter and place in the oven for around 45 minutes to an hour. I’ll generally set an initial timer to 35 minutes and check on it periodically.
    &lt;ul&gt;
      &lt;li&gt;Note: if you’re using a pan with a wider base, I would check on it more often.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Let it cool for around 20 minutes, and if you aren’t serving it right away, cover with alumnium foil to avoid the bread from drying out.&lt;/li&gt;
  &lt;li&gt;Enjoy! :D&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;so-why-oat-flour&quot;&gt;So Why Oat Flour?&lt;/h4&gt;

&lt;p&gt;The reason I prefer to use oat flour boils down to how the grain is processed to make regular flour. The grain in all purpose flour is processed in a way that it loses most of its nutrients and fiber. Oats, on the other hand, are considered a whole grain which means it goes through less processing and holds onto most of its fiber and nutrients. This not only provides important nutrients to your body, but also can help keep you full for longer! Whole wheat flour has similar properties to making oat flour, but it tends to be a little more expensive. Using just a blender or food processor, you can ensure you’re giving your body the nutrients you need without spending too much!&lt;/p&gt;</content><author><name></name></author><summary type="html">Prep Time: 15 minutes</summary></entry><entry><title type="html">Eat to Beat Disease: a MUST READ</title><link href="http://localhost:4000/food_for_thought/eat-to-beat-disease/" rel="alternate" type="text/html" title="Eat to Beat Disease: a MUST READ" /><published>2019-09-05T08:25:40-05:00</published><updated>2019-09-05T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/eat-to-beat-disease</id><content type="html" xml:base="http://localhost:4000/food_for_thought/eat-to-beat-disease/">&lt;p&gt;The other day I was at the library, looking for a quick read before I head off to school in a week, and I spotted a thick book with color foods on the front titled &lt;em&gt;Eat to Beat Disease&lt;/em&gt;. ‘This sounds interesting!’ I thought to myself, and as I was in a hurry to leave, I quickly checked it out. That night, I started reading after dinner and literally could not stop. This book is everything I’ve been looking for! And I’m not exaggerating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/food_for_thought/assets/images/blog/food/eat_to_beat.jpg&quot; alt=&quot;Selfie with my favorite new book&quot; class=&quot;images third&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;caption&quot;&gt;Here it is! My favorite new read&lt;/p&gt;

&lt;p&gt;For the longest time, I wanted a resource (a collection of journal articles, podcasts, book, &lt;em&gt;anything!&lt;/em&gt;) that explained why some foods are healthy and others aren’t with scientific backing, and this book &lt;span class=&quot;standOut&quot;&gt;does it all&lt;/span&gt;. It explains why certain foods are good for you beyond the scope of how full it makes you or whether the calories are “worth it”. In fact, I’m currently about halfway through the book, and the author, Dr. William Li, has barely even talked about calories! WAKE UP AMERICA! Calories are &lt;strong&gt;not&lt;/strong&gt; the only measure of health!&lt;/p&gt;

&lt;p&gt;As a quick sneak peek, Dr. Li first explains five of the body’s amazing defense mechanisms against diseases including cancer, diabetes, heart disease, and many others that seem to only be treatable by modern medicine. The five mechanisms are summarized as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Angiogenesis: building blood vessels to various parts of the body&lt;/li&gt;
  &lt;li&gt;Regeneration: using stem cells to repair injuries&lt;/li&gt;
  &lt;li&gt;Our gut’s microbiome: the good bacteria living inside of us&lt;/li&gt;
  &lt;li&gt;DNA Projection and Repair: affecting expression of certain genes&lt;/li&gt;
  &lt;li&gt;Immunity: using our immune system to build resistance against certain diseases/invaders.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Except, read the book for a much more thorough and in depth explanation of the mechanisms; this is barely the tip of the iceberg! Then, Dr. Li goes on to explain which foods affect each of the five mechanisms and guess what! They’re mostly foods that you’re probably already eating! He backs up each food listed with scientific evidence, whether from clinical trials or experiments done in a lab setting, and usually uses more than 1 experiment as an example.&lt;/p&gt;

&lt;p&gt;For me personally, before picking up this book, I kind of judged foods on if they were healthy or not based on if they had a lot of fiber and would fill me up while minimizing the number of calories I consumed. However, this is such a black and white scale for a field as colorful as a rainbow, almost literally! There’s so much more to health than just digestion rate. For example, it’s known that cheese is pretty calorie/energy dense and contains quite a bit of saturated fat, so I tended to avoid it as much as I can. However, cheese (especially European cheese) has a lot of probiotics and prebiotics (good bacteria and food for the bacteria in our microbiome, respectively) that will improve our overall health! In moderation, of course, cheese is really beneficial!&lt;/p&gt;

&lt;p&gt;I’m already looking at the foods I eat and making a mental note of which defense mechanism I’m improving by eating this, but the list is so long, I’ll definitely have to give &lt;em&gt;Eat to Beat Disease&lt;/em&gt; another read to understand more!&lt;/p&gt;

&lt;p&gt;As summer turns into Fall and we’re stuck inside anyways, &lt;strong&gt;definitely&lt;/strong&gt; put this on your read list!&lt;/p&gt;

&lt;p&gt;Note: it is a little scientifically dense, and while Dr. Li does a great job of explaining it, if a concept is new, don’t be afraid to re-read a part a few times or use Google! I know I definitely did :)&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">The other day I was at the library, looking for a quick read before I head off to school in a week, and I spotted a thick book with color foods on the front titled Eat to Beat Disease. ‘This sounds interesting!’ I thought to myself, and as I was in a hurry to leave, I quickly checked it out. That night, I started reading after dinner and literally could not stop. This book is everything I’ve been looking for! And I’m not exaggerating.</summary></entry><entry><title type="html">Sure about sugar?</title><link href="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar.html" rel="alternate" type="text/html" title="Sure about sugar?" /><published>2019-08-27T00:00:00-05:00</published><updated>2019-08-27T00:00:00-05:00</updated><id>http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar</id><content type="html" xml:base="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar.html">&lt;h5 id=&quot;all-sugar-is-bad-right&quot;&gt;All sugar is bad right?&lt;/h5&gt;

&lt;p&gt;Okay that’s a tough question. Sugar by itself is not necessarily bad; it’s a good source of quick energy for our bodies. &lt;em&gt;HOWEVER&lt;/em&gt;, too much sugar, especially in its most processed form can be pretty detrimental to our bodies and lead to weight gain as well as medical issues.&lt;/p&gt;

&lt;h5 id=&quot;how-does-our-body-digest-sugar&quot;&gt;How does our body digest sugar?&lt;/h5&gt;

&lt;p&gt;Basically, sugar will be broken down to glucose, which is the form of energy our cells require, and the glucose gets sent to one of three places:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Our cells, to give them energy&lt;/li&gt;
  &lt;li&gt;Our liver, to store glucose for later in the form of glycogen (which you don’t need to worry about)&lt;/li&gt;
  &lt;li&gt;Our adipose tissue as fat.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our cells prefer to have sugar because they can quickly be broken down into glucose, but our cells can only take so much glucose at a time. Same with our liver. The first two have a capacity of how much glucose gets sent there, but adipose tissue &lt;span class=&quot;standOut&quot;&gt; can take as much glucose as it wants&lt;/span&gt;. That’s where the dangerous part is.&lt;/p&gt;

&lt;p&gt;If our body breaks down too much sugar at once and has too much glucose for our cells and liver, it will send all the excess glucose to turn into fat, which will lead to weight gain.&lt;/p&gt;

&lt;h5 id=&quot;so-we-should-avoid-all-sugar&quot;&gt;So we should avoid all sugar?&lt;/h5&gt;

&lt;p&gt;Not necessarily! The main difference in sugar that is, for example, added in candy and sodas, and sugars in whole fruit is the magical thing called &lt;span class=&quot;standOut&quot;&gt; fiber&lt;/span&gt;. The sugars in fruit are all encased by a lot of fiber, which if you recall, slows down the digestion of food. This means that the sugar in fruit gets slowly broken down and transported to the appropriate location over a longer period of time. If you ate a candy bar, on the other hand, basically all that sugar gets broken down to glucose and taken to the appropriate destination all at once. Sugars in fruit are able to provide your &lt;em&gt;cells&lt;/em&gt; with a more constant form of energy, which also helps you feel fuller and maintain energy.&lt;/p&gt;

&lt;p&gt;In fact, fiber is actually the main difference between white sugar and brown sugar. Although brown sugar is not the same as the sugar we get in whole fruits, it has more fiber compared to white sugar, and thus is marketed as healthier.&lt;/p&gt;

&lt;h5 id=&quot;how-do-we-know-what-has-added-sugar&quot;&gt;How do we know what has added sugar?&lt;/h5&gt;

&lt;p&gt;Ah yes this can be the tricky one. The first place I would check is the nutrition label. Under ‘sugar’, some labels have a row for ‘added sugar’ which is not very desirable. However, for a more thorough check, you must go to the ingredient list. &lt;span class=&quot;standOut&quot;&gt;But sugar is tricky.&lt;/span&gt; After people realized sugar was leading to weight gain and other health complications, many companies became sly about hiding sugar into their products by using different names for sugar but in reality, it just means ‘Added Sugar’. Some common ones to look out for are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The mono and disaccharides (dextrose, fructose, galactose, glucose, lactose, maltose, and sucrose)&lt;/li&gt;
  &lt;li&gt;Cane sugar, evaporated cane juice&lt;/li&gt;
  &lt;li&gt;High Fructose Corn Syrup&lt;/li&gt;
  &lt;li&gt;Corn syrup solids&lt;/li&gt;
  &lt;li&gt;Dextrin&lt;/li&gt;
  &lt;li&gt;Glucose syrup solids&lt;/li&gt;
  &lt;li&gt;Maltodextrin&lt;/li&gt;
  &lt;li&gt;Agave Nectar/Syrup (very similar to honey)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;im-never-eating-sweets-again-right&quot;&gt;I’m never eating sweets again, right?&lt;/h5&gt;

&lt;p&gt;I’d say enjoy in moderation! It’s very difficult to cut out something entirely from your diet because you’ll probably start to crave it even more, causing a binge eating session. It’s okay to enjoy that sweetness of a candy bar or ice cream once in a while, but be mindful of what’s happening inside your body after you eat it.&lt;/p&gt;</content><author><name></name></author><category term="nutrition" /><summary type="html">All sugar is bad right?</summary></entry><entry><title type="html">Week 9: Wrapping Things Up!</title><link href="http://localhost:4000/food_for_thought/FM-week-9/" rel="alternate" type="text/html" title="Week 9: Wrapping Things Up!" /><published>2019-08-26T20:31:00-05:00</published><updated>2019-08-26T20:31:00-05:00</updated><id>http://localhost:4000/food_for_thought/week-9</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-9/">&lt;h3 id=&quot;monday-august-26-2019&quot;&gt;Monday August 26, 2019&lt;/h3&gt;

&lt;p&gt;Happy Monday! This past weekend was so relaxing! It was kind of hard to get up for work today haha. And I faced a bit of a surprise when I arrived at the museum because my badge didn’t work anymore! I was supposed to end the internship last week, but we extended it for one week and we didn’t let Access Control know… oopsies&lt;/p&gt;

&lt;p&gt;It’s all fixed now, don’t worry :D&lt;/p&gt;

&lt;p&gt;After that mini roadblock, I continued to examine the models and make changes to improve them. In my model training to find the differences between Lycopodium and Selaginella, some folds did really well and training accuracy reached 90% while validation accuracy lagged behind, but in other folds, the training &lt;em&gt;and&lt;/em&gt; validation accuracy both plummeted to around 50% by the 10-15 epoch and just couldn’t recoveer. In all epochs, however, loss was constantly decreasing. I’m not really sure what’s happening there and why it would happen on some folds and not others… maybe there’s a few images that are just really throwing the model off? Although I did look through them manually before. Perhaps it’s worth another look.&lt;/p&gt;

&lt;p&gt;I first looked into doing image augmentation, but I originally thought that it would result in the model training on &lt;em&gt;more&lt;/em&gt; images than you have every epoch. Turns out, it just trains on a smaller sample, but each epoch it uses &lt;em&gt;randomly altered&lt;/em&gt; images that are different to the model. It may be useful in another application (like the frullania), but currently when we have so many images already, I don’t think we need it.&lt;/p&gt;

&lt;p&gt;I tried playing around with the regularizers today in the Dense and Convolutional-2D (Conv2D) layers, but most combinations I tried resulted in the model overfitting very early and the training and validation accuracy would begin to decrease by the 6-7th epoch. (To see what changes I made, check out &lt;a href=&quot;https://docs.google.com/spreadsheets/d/15972K_rdv3zpnvnV--wSaTpiK0jnVDkBzhUWjre5BLg/edit?usp=sharing&amp;quot;&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;this doc&lt;/a&gt;. The model started to learn again pretty well when I kept activity regularizers in the Dense layers, despite there not being an exact equivalent in the Smithsonian Methematica Model.&lt;/p&gt;

&lt;p&gt;We’ll see what happens tomorrow morning!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;p&gt;PS, Matt wants me to try running the model again on the two frullania species again but the images being from farther away. I want to look into bootstrapping for this and hopefully synthetically expand our data set.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-28-2019&quot;&gt;Tuesday, August 28, 2019&lt;/h3&gt;

&lt;p&gt;First thing this morning, I checked on the training of the model on Lycopodium and Selaginella. For the most part, it was okay; there was still a bit of overfitting, but the trend of validation accuracy seemed to follow training accuracy, like in the graph below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/food_for_thought/assets/images/blog/fm/val_accuracy_1.png&quot; alt=&quot;Slightly overfit accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, on other folds, we got something disastrous like this:
&lt;img src=&quot;/food_for_thought/assets/images/blog/fm/val_accuracy_8.png&quot; alt=&quot;Dropped accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m not entirely sure why on most folds, the model seems to be training well, while on others, it’s completely unable to after a certain amount of time. I don’t think it’s overfitting because even the training accuracy decreased. Well, tomorrow, I’m meeting with Dr. Iacobelli, the computer science professor at Northeastern Illinois University, so hopefully he’ll be able to offer more insight.&lt;/p&gt;

&lt;p&gt;Additionally, I received new images of the frullania species today. These ones are more zoomed in and hopefully will allow the model to pick up on more details in the differences between the two plants. I implemented some bootstrapping to train the model because we only had a little less than 300 images which I’m not sure is enough to be able to completely train the model on. I left it going overnight, so I’ll be able to check in on Thursday.&lt;/p&gt;

&lt;p&gt;At this point, most of my big machine learning changes are wrapping up and I’ve been working on documentation, documentation, and documentation! Especially since I was pretty much the only one working on this project this summer, it’s &lt;strong&gt;extremely&lt;/strong&gt; important that someone could study my work for a bit and pick up where I left off.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-28-2019&quot;&gt;Wednesday August 28, 2019&lt;/h3&gt;

&lt;p&gt;This morning I met with Dr. Iacobelli, Beth, and Dr. Campbell (unfortunately Matt coudln’t make it) We updated Dr. Iacobelli and here are his responses/advice. Regarding the random dropped accuracy I wrote about yesterday, he recommends that we meticulously check the code first. There may a bug (such as divide by 0) that’s throwing everything off. Additionally, it simply may be lack of processing power in the computer causing a data overflow.&lt;/p&gt;

&lt;p&gt;Another thing he noticed is that the images we’re using from the Field Musuem are &lt;em&gt;not&lt;/em&gt; consistent in the herbarium sheet layout. Some have labels in the upper right corner, some are lower left, some have specimen in the middle, some have specimen in the lower right corner. The inconsistencies in layout may be providing too much noise for the model, so he recommends that we try a smaller sample of images that have a more consistent layout an see if there is an improvement in performance.&lt;/p&gt;

&lt;p&gt;Additionally, there is a computer at NEIU with 64 GB RAM that Beth is going to be trying our model with the Smithsonian images (which I didn’t know we had!) and we found out there is a computer at the museum with &lt;span class=&quot;standOut&quot;&gt; one terabyte of RAM. I repeat, of RAM. &lt;/span&gt; I almost fainted when I heard that.&lt;/p&gt;

&lt;p&gt;Although I’m wrapping up this week, I’m excited for Beth to be able to try things out!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-29-2019&quot;&gt;Thursday August 29, 2019&lt;/h3&gt;

&lt;p&gt;When I came in this morning, the model that was training on images of the Adiantum and Blechnum genera finished training, so I created a script to test it. Since the model really consists of 10 trained models (given that we used 10 fold cross validation), I had to take that into account. One way to do this would be see which prediction output each model gave and the final prediction would be what the majority of the models output. However, this poses two possible issues:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What if 5 models say class A and 5 say class B? There wouldn’t be an odd number in this case to be the tie breaker&lt;/li&gt;
  &lt;li&gt;If one model is 99% sure that an image is class A but another model is only 50% sure that it’s class A, using the method described above would give each of these equal weights when it maybe shouldn’t.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To overcome this, I decided to take a step back and look at the probability outcomes that are used to determine each model’s final answer. Basically, the final layer of the model outputs two numbers: the probability that an image is one class or another. So what I did was I kept track of each of these probabilities and for each image, added up all the models’ class A probability and class B probability separately. The final answer would be whichever summed probability is higher. This way, the “confidence” of each of the 10 models is taken into account.&lt;/p&gt;

&lt;p&gt;Honestly, that didn’t take me very long today, so when I tested the Adiantum/Blechnum model, we got a whooping &lt;span class=&quot;standOut&quot;&gt;97.8% accuracy!!&lt;/span&gt; It was tested on 400 new images and this is the best I’ve ever gotten :)))&lt;/p&gt;

&lt;p&gt;This is good news because it means the model can work! We just need to tweak values and perhaps look at cleaning our input images for Lycopodium and Selaginella.&lt;/p&gt;

&lt;p&gt;The rest of the day was mainly spent documenting my work. I explained to Matt how my Google Drive was organized and how to use Github as well as documented within the Drive where everything is.&lt;/p&gt;

&lt;p&gt;I did, however, forget to upload my final cross-validation testing file to Github, which is what I’ll do first thing tomorrow morning.&lt;/p&gt;

&lt;p&gt;Last day tomorrow! What a melancholy feeling…&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-30-2019&quot;&gt;Friday August 30, 2019&lt;/h3&gt;

&lt;p&gt;Today was spent mainly double checking my documentation on Github and Google Drive. I called Beth and we went over each file in Github and she seems confident on where I’m leaving off, which is great! I’m so excited for her to continue this project and be able to run the model on a computer that has enough processing power.&lt;/p&gt;

&lt;p&gt;Furthermore, Matt worked on getting the images contrasted a bit more yesterday; now the background of the images are very white compared to the specimen in focus and it helped get rid of some noise. We only have a little less than 150 of either group, but we ran the model for 50 epochs and we actually got some results! Although overfitting still occurred, the validation accuracy definitely grew as the number of epochs increased! This is a great sign! It means the machine is able to generalize what it’s learning on the training data onto new images.&lt;/p&gt;

&lt;p&gt;Since we didn’t run for very long and we don’t have many images, we were able to start another run before the end of the day. Matt went through the images and deleted any that were actually the wrong specimen or weren’t sterile as this type of noise could be hurting the model’s performance. Then, before I left, I ran two models: one with the same exact specs as the last run, just with the new images, and one with the new images &lt;em&gt;and&lt;/em&gt; more epochs. Although I won’t be able to see the results, Matt will be able to check it out and report back!&lt;/p&gt;

&lt;p&gt;And that’s a wrap :) although I’m leaving, I know this project is going to go far in the hands of Beth, Matt, and Dr. Iacobelli. I’m excited to see where they are able to take this application of computer science into botany and am truly grateful for this opportunity!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 26, 2019</summary></entry><entry><title type="html">Becoming Keen on Quinoa</title><link href="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa.html" rel="alternate" type="text/html" title="Becoming Keen on Quinoa" /><published>2019-08-20T00:00:00-05:00</published><updated>2019-08-20T00:00:00-05:00</updated><id>http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa</id><content type="html" xml:base="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa.html">&lt;h5 id=&quot;sooo-what-is-quinoa&quot;&gt;Sooo what is quinoa?&lt;/h5&gt;

&lt;p&gt;Quinoa (pronounced keen-wah) is a type of grain that is in my opinion both delicious and extremely nutritious. Uncooked, they look like little seeds, normally yellow or brown, but after boiling, they become soft and fluffy. In addition to being an excellent source of carbs and energy, they have a surprising amount of protein and are even&lt;span class=&quot;standOut&quot;&gt; a complete protein&lt;/span&gt;! Meaning quinoa contains &lt;strong&gt;all nine&lt;/strong&gt; essential amino acids.&lt;/p&gt;

&lt;h5 id=&quot;but-why-is-it-better-for-me-than-flour-or-rice&quot;&gt;But why is it better for me than flour or rice?&lt;/h5&gt;

&lt;p&gt;For a few reasons, actually. First quinoa is not as processed as flour and thus retains more of its vitamin and minerals as well as fiber. Compared to rice, quinoa’s glycemic index is significantly lower (53 compared to 73 of white rice) which means that it does not produce as big of an insulin spike. (Which is good! We don’t want to unnecessarily tire out our pancreas.) The main reason for this is &lt;span class=&quot;standOut&quot;&gt; fiber&lt;/span&gt;!!&lt;/p&gt;

&lt;p&gt;Recall that fiber is essentially a food’s “protective barrier” to being digested. But of course, our digestive system always overcomes it. Fiber itself is undigestable and slows down our body’s ability to digest the food it encases. This may sounds bad, but it’s actually extremely good, especially for individuals who are wary of caloric intake. The fiber helps us eat the same number of calories, but feel fuller for longer, reducing the cravings we often feel after eating pasta or other refined grains.&lt;/p&gt;

&lt;p&gt;If you take away anything from this blog post, I hope it’s that &lt;span class=&quot;standOut&quot;&gt; quinoa = fiber! &lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&quot;what-do-you-even-eat-it-with&quot;&gt;What do you even eat it with?&lt;/h5&gt;

&lt;p&gt;I love quinoa as a rice/grain substitute! Maybe it’s the Asian side in me, but I’ll often have a bowl of quinoa with my dinner or lunch just instead of rice. Additionally, it’s really good in salads and or various types of bowls. It’s really quick and easy to cook a bunch of quinoa at once and refrigerate; it can probably last a good week or so!&lt;/p&gt;

&lt;h5 id=&quot;im-a-broke-college-student-how-can-i-afford-this&quot;&gt;I’m a broke college student, how can I afford this?&lt;/h5&gt;

&lt;p&gt;Admittedly, quinoa can be more expensive than some easier foods, such as pasta or rice. But remember, nutrition is about a long run investment. Although you may be spending a bit more now, you’re treating your body better (which in itself is priceless!) and maybe even saving money on future medical bills. Plus, a small bag of quinoa actually makes quite a lot, I think you’d be surprised :)&lt;/p&gt;

&lt;h5 id=&quot;how-do-you-cook-quinoa&quot;&gt;How do you cook quinoa?&lt;/h5&gt;

&lt;p&gt;Easy! Simply put about 1 cup of quinoa and 1 cup of water in a pot and bring to a boil. If you’re cooking less, I would put a little more water, about a 1:1.25 quinoa to water ratio. Once it starts to boil, turn down the heat and cover the pot, allowing the quinoa to simmer. Once most of the water is gone, I would check on it frequently and stir to prevent any quinoa from sticking to the bottom.&lt;/p&gt;

&lt;p&gt;The quinoa is cooked if it looks almost “fluffy” and it shouldn’t be crunchy anymore.&lt;/p&gt;</content><author><name></name></author><category term="nutrition" /><summary type="html">Sooo what is quinoa?</summary></entry><entry><title type="html">Week 8: Implementing Cross Validation</title><link href="http://localhost:4000/food_for_thought/FM-week-8/" rel="alternate" type="text/html" title="Week 8: Implementing Cross Validation" /><published>2019-08-19T08:25:40-05:00</published><updated>2019-08-19T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-8</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-8/">&lt;h3 id=&quot;monday-august-19-2019&quot;&gt;Monday August 19, 2019&lt;/h3&gt;

&lt;p&gt;This morning, Beth came to the Field Museum to work with me on implementing stratified cross validation and ROC testing. Stratified cross validation essentially partitions the data into groups of approximately equal distributions of each class. ROC (stand for Receiving Operator Curve) is a type of graph that can help us quantify how “good” our model is. It uses the percentage of true positives and false positives and the closer the curve hugs the y-axis, the more confident it is. Take a look at the image below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/food_for_thought/assets/images/blog/fm/ROC_curve.png&quot; alt=&quot;ROC Curve&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The straight line represents if it were just up to chance, so if the ROC curve is close to that, it’s bad.&lt;/p&gt;

&lt;p&gt;I got the code to run with stratified cross validation and creating an ROC curve at the end with small amounts of bogus test data, but when I tried to import larger amounts of useful data, the program seems to be crashing and I’m not sure why. I ran the old model again (prior to implementing cross validation) and it seemed to work okay, but in this new script, it feels like everything’s falling apart (rip). Before the program started crashing, it would train but ridiculously overfit on the training data while validation accuracy didn’t increase at all :(&lt;/p&gt;

&lt;p&gt;I’m trying to see if I accidentally changed something when adapting the model, but then the code crashed entirely, freezing the computer.&lt;/p&gt;

&lt;p&gt;This is definitely a rough note to end the day on, but I’m going to try not to let it bother me too much and come in tomorrow ready to problem solve!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-20-2019&quot;&gt;Tuesday August 20, 2019&lt;/h3&gt;

&lt;p&gt;This morning, I was able to fix why the code would crash my computer! When I was using the split function on the stratifiedKFold object and accessing features and labels that made up the training and validation sets, they had to be numpy arrays. My labels data structure was only a list and thus would throw an error and crash it. After making a few more tweaks on random seeds and epochs, aroudn 10:30 AM I started running the 10 fold cross validation and it hasn’t stopped since. I’ve left it on and hope no one messes with it! Tomorrow morning, we’ll be able to check out results and see what changes we can make to improve the model.&lt;/p&gt;

&lt;p&gt;While the model was training, I was tasked with a side task. There’s a bryophyte checklist of the Java region that’s written as a word document. We want to parse the document and store the genus, species, and founder into a CSV. Currently, I’m using a package called python-docx and basic python string manipulation to parse out the desired information. The only downside is this code I’m writing is only specific to this document which of course isn’t as fun in CS.&lt;/p&gt;

&lt;p&gt;Have a great day!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-21-2019&quot;&gt;Wednesday August 21, 2019&lt;/h3&gt;

&lt;p&gt;Today was a &lt;strong&gt;long&lt;/strong&gt; day, oof. To start out, Matt and I talked about the project and thought about why our results weren’t exactly matching that of the Smithsonian. First, and foremost, it’s most likely because of our data that we’re using. The Smithsonian is using images from their own herbarium while we’re using images from the Field Museum herbarium and obviously those will produce different results. Within the family of Lycopodiaceae, the distribution of images across genera are different between their herbarium and ours. Additionally, they were able to use a lot more images due to processing power. I found out today, that using around 4000 images is a lot for the computers we have (i5 Intel Cores with 8 GB RAM). Sometimes the computer just freezes entirely and you have to hard restart it. Thus, I decreased my number of images to around 3300 and it seems to be running okay!&lt;/p&gt;

&lt;p&gt;Additionally, instead of comparing families of plants (Lycopodiaceae and Selaginellaceae), we decided to simplify the inputs to two genera of plants (Lycopodium and Selaginella). Selaginella is the only genus in the family Selaginellaceae (as a reminder the hierarchy goes family &amp;gt; genus &amp;gt; species) and Lycopodium is one of around 8 or so genera in the family Lycopodiaceae. Hopefully this will provide less noise in the first class and make it easier for the machine to detect prominent differences.&lt;/p&gt;

&lt;p&gt;Additionally, just for kicks, Matt wanted me to run the model to compare two new and different genera: adiantum and blechnum. This is because these two families look VERY different to the eye, so the model should do a relatively good job with this. I spent most of today preparing the images to be run on a different computer and started running the model before I left.&lt;/p&gt;

&lt;p&gt;I got in around 8:15 this morning and didn’t leave till around 5:30, I’m going to go sleep a lot now!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-22-2019&quot;&gt;Thursday August 22, 2019&lt;/h3&gt;

&lt;p&gt;I came in this morning and good news, nothing crashed! The models are still chugging along doing their own thing. Something I noticed and continue to notice is that while training accuracy will rise somewhat consistently with each epoch, validation accuracy fluctuates a LOT more, like in the graph below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/food_for_thought/assets/images/blog/fm/8.14.19_12.30.png&quot; alt=&quot;Fluctuating Validation Accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m going to look into this more, but after a quick Google search, it seems one issue may be too much dropout. So after this model is finished training, I’m going to look into making that change (which might end up having to be done tomorrow)!&lt;/p&gt;

&lt;p&gt;The program I started running yesterday with Lycopodium and Selaginella was taking forever… after 18 hours, it only completed 5 folds of the 10 fold cross validation and Beth and I decided that it was too long to realistically experiment. Thus, we decreased the images from 256 x 256 pixels down to 128 x 128 and it seems to be going much faster! Each epoch now takes a little over a minute instead of around 10 minutes.&lt;/p&gt;

&lt;p&gt;Additionally, I’m running a baseline test with the model and images of Frullania rostrata and Frullania coastal. As a reminder, these are two possibly distinct species that we want to gather further evidence that they are different. Although the model isn’t “tuned” to picking up the differences in the Frullania, it’ll be nice to have a baseline essentially and see how to go from there. Beth had also mentioned that there’s a way to see what features each layer of the model is picking up, which would be helpful in our case to see what layers are working for our purpose and what should be changed.&lt;/p&gt;

&lt;p&gt;Take it easy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;h3 id=&quot;friday-august-23-2019&quot;&gt;Friday August 23, 2019&lt;/h3&gt;

&lt;p&gt;Happy Friday! When I came in this morning, all three of my models finished running and here’s a brief summary of the results:&lt;/p&gt;

&lt;p&gt;The Lycopodium/Selaginella results: The validation accuracy followed the training accuracy, but there was still a lot of fluctuation. In my next training, I increased batch size from 32 to 64 and increased epochs to 100. May as well since we have the weekend to run it. Before I left for the day, it finished training 2/10 folds and I noticed in the second fold, by the end neither training nor validation accuracy increased from 50%. This leads me to believe we shouldn’t increase batch size much above 32.&lt;/p&gt;

&lt;p&gt;The Adiantum/Blechnum results: This run was mostly as a baseline to see whichout changing the model, what results did we get. Again, they were pretty similar to before: validation accuracy seemed to follow training accuracy but with a LOT more and larger fluctuations. In this next training session, I decreased the initial dropout layer from 50% to 40% to see if perhaps less measures against overfitting would help.&lt;/p&gt;

&lt;p&gt;The Frullania (coastal/rostrata) results: awful. I really didn’t expect much but what we saw here in all the folds was a classic case of overfitting. The training accuracy kept increasing while validation accuracy remained fluctuating around 50%. To make this work, we would need to reconfigure the layers of the model to “detect” these certain features.&lt;/p&gt;

&lt;p&gt;On a more fun note, today our department had a large rpotluck! There were all sorts of food from Malaysian, Latin American, to Chinese! I brought carrots and hummus and a Chinese Eight Treasures Cake which got really good feedback! I ate literally so much food but it was absolutely amazing :)&lt;/p&gt;

&lt;p&gt;Have a great weekend in this beautiful weather!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 19, 2019</summary></entry><entry><title type="html">Week 7: Improving CNN Training</title><link href="http://localhost:4000/food_for_thought/FM-week-7/" rel="alternate" type="text/html" title="Week 7: Improving CNN Training" /><published>2019-08-12T08:25:40-05:00</published><updated>2019-08-12T08:25:40-05:00</updated><id>http://localhost:4000/food_for_thought/week-7</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-7/">&lt;h3 id=&quot;monday-august-12-2019&quot;&gt;Monday August 12, 2019&lt;/h3&gt;
&lt;p&gt;Today was essentially spent just making changes to the architecture/regularizers of my model and training it to hopefully get better results. The main issues I’m facing now is that validation accuracy fluctuates a lot. It’ll grow steadily for a little then drop down to almost 50% and grow a little more (such as in the image below).
&lt;img src=&quot;/food_for_thought/assets/images/blog/fm/8.13.19_13.47.png&quot; alt=&quot;Accuracy Graph&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although it took all day because training the model currently takes around an hour, the main things I experimented with were changing the epsilon value (which prevents dividing by 0) and the regularizers. Regularizers essentially penalize larger weights, forcing the parameters to be small. I used L2 Regularization which adds a penalty term to the loss function that’s composed of the sum of the squares of the parameters.&lt;/p&gt;

&lt;p&gt;The validation accuracy is still fluctuating a lot but I just called it a day after working on this for a long time.&lt;/p&gt;

&lt;p&gt;See you tomorrow!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;tuesday-august-13-2019&quot;&gt;Tuesday August 13, 2019&lt;/h3&gt;
&lt;p&gt;Continuing the work of yesterday, I was a little frustrated with why the validation accuracy was so awful. Doing some more comparisons between my model architecture and the Smithsonian, I did notice a discrepancy. In the Smithsonian paper (which as a reminder used Mathematica, not Python) had layers called ‘linear layers’ which simply output weights*inputs+bias. I assumed a Dense layer in Keras did the same, but after doing some research, I found that I was wrong. I added the linear activation function to these layers and the results seemed to be at least a little more consistent in terms of validation accuracy/loss.&lt;/p&gt;

&lt;p&gt;Although it’s still far from perfect, the validation losses and accuracy seem to be following the training loss and accuracy a little more closely. I believe the fluctuations mean that the model is still overfitting to training data and is unable to be consistent with the validation cases. The next step I want to try is incorporating more drop out or possibly increasing regularizers. I do need to be careful with regularizers though; I don’t want to have too much and prevent the model from continuing to learn.&lt;/p&gt;

&lt;p&gt;In addition to using a model to learn between Selaginellaceae and Lycopodiaceae, we want to see if we can apply the same model architecture to two possibly different species of a plant under Frullania. This would be a completely new application and really cool! But still in the process of obtaining images right now. One thing that will be kind of difficult with that is we only have around 100 images of each while with the families I’m working with now, there are thousands and I’m still struggling to get a good model.&lt;/p&gt;

&lt;p&gt;Wish me luck, I’ll need it.&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-14-2019&quot;&gt;Wednesday August 14, 2019&lt;/h3&gt;

&lt;p&gt;This morning was a bit slow because training the model currently takes a very long time, but I added another dropout layer which seems to help with overfitting! Validation accuracy is fluctuating just a little bit less :) Additionally, I found that for some reason, when I end training and resume it again at the same epoch, it produces different results than if I don’t stop. This leads me to believe there is something that I haven’t been able to control in regards to the randomness of the model and it may be causing misleading results. This is something I’d like to look more into.&lt;/p&gt;

&lt;p&gt;Additionally, I received images today of two different species that Dr. Matt von Konrat (aka my boss) has been doing research on. They’re called frullania coastal and frullania rostrata (you may recall what they look like from previous posts about Morphosnake). Right now, we’re trying to build up the amount of evidence to show that these in fact are different species. However, we only have about 90-100 of each species, so I’m currently looking into using image augmentation with Keras to give ourselves more images and prevent overfitting. As a reference for myself, I’m using &lt;a href=&quot;https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/&quot; class=&quot;standOut&quot;&gt;this website&lt;/a&gt; as a reference. I can also look into changing my model to help against overfitting. For example, in the other project, my batch size was 32, but since I only have about 130 images total to train with right now, decreasing my batch size to 8 already improved validation accuracy without sacrificing too much time.&lt;/p&gt;

&lt;p&gt;This is something new and exciting to me! Hopefully we can get some results by early next week :)&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-july-15-2019&quot;&gt;Thursday July 15, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I worked on a data summary script to finalize a project that the high school interns worked on. As a reminder, their project was a community science activity on Zooniverse that allowed people to examine fern specimen. We wanted to see in general how close the community user responses were to an expert response (done by our very own Dr. Matt von Konrat). I worked on a script that took the exported data from Zooniverse and parsed through it. Although it wasn’t difficult, it took a while because Jessica (another intern!) and I kept chatting (oops!)&lt;/p&gt;

&lt;p&gt;After that, I started working back on the machine learning project. Last night, I called with Dr. Francisco Iacobelli, a computer science professor at Northeastern Illinois University. We talked about implementing k-fold Cross Validation methods to test the robustness of a model. The idea is that you take all your images and split it into 10 groups randomly. Choose one group to be the validation data and the successive group to be the testing data. The remaining data acts as training data. Train the model and save it, then repeat so all groups get a chance to be the testing data. In the end you have 10 trained models that really make up &lt;strong&gt;the model&lt;/strong&gt;. This makes a lot more sense to me after listening to Dr. Iacobelli talk about it; I had seen it online but for some reason it didn’t really stick until now.&lt;/p&gt;

&lt;p&gt;By the end of the day, I think I almost finished implementing the cross validation! I’m currently in the process of writing the loop that partitions the training, testing, and validation data, and after that, I just have to run and save the model for each group.&lt;/p&gt;

&lt;p&gt;I won’t be at the museum tomorrow, but I’ll try to get some work done on my own, mostly just documentation so I don’t have to do that later.&lt;/p&gt;

&lt;p&gt;Happy Thursday everyone!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-16-2019&quot;&gt;Friday August 16, 2019&lt;/h3&gt;

&lt;p&gt;Today I didn’t go to the museum, I had a meeting at Northeastern Illinois University with our grad student Beth, Dr. Campbell (a biology professor who’s been working with some of our summer interns), and Jose. We basically just updated everyone else where we are on the project and Beth and I communicated a bit more on how we can work together. She’s been pretty busy with other projects at the same time, so this was a good opportunity for us to touch base.&lt;/p&gt;

&lt;p&gt;After the meeting, it wasn’t worth it for me to go back to the Field Museum, so I went home and worked on doing some documentation. I updated the ReadMe file in &lt;a href=&quot;https://github.com/allisonchen23/ml_classifications&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;my Github for this machine learning project&lt;/a&gt;. Documentation is something I’ll really have to be on top of things about especially because not many people in this department are familiar with using python and machine learning.&lt;/p&gt;

&lt;p&gt;Although not terribly busy, today was a good day to prep me for the work I need to do next week!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 12, 2019 Today was essentially spent just making changes to the architecture/regularizers of my model and training it to hopefully get better results. The main issues I’m facing now is that validation accuracy fluctuates a lot. It’ll grow steadily for a little then drop down to almost 50% and grow a little more (such as in the image below).</summary></entry></feed>