<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/food_for_thought/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/food_for_thought/" rel="alternate" type="text/html" /><updated>2019-09-14T08:51:17-07:00</updated><id>http://localhost:4000/food_for_thought/feed.xml</id><title type="html">Thoughts for Food</title><subtitle>Welcome to my blog! I'll be posting basically random thoughts and updates on any current projects/work I'll be doing.</subtitle><entry><title type="html">Understanding People (well, trying to)</title><link href="http://localhost:4000/food_for_thought/understanding_people/" rel="alternate" type="text/html" title="Understanding People (well, trying to)" /><published>2019-09-12T04:00:40-07:00</published><updated>2019-09-12T04:00:40-07:00</updated><id>http://localhost:4000/food_for_thought/understanding_people</id><content type="html" xml:base="http://localhost:4000/food_for_thought/understanding_people/">&lt;p&gt;This topic has been something that really interested me this summer. I talked to my mom a lot about it and from her, realized that working effectively with people is probably one of the hardest things we have to learn to do. It’s not something you can only read from a textbook and understand immediately; it takes many experiences and practice to learn how to work with people without offending or hurting anyone.&lt;/p&gt;

&lt;p&gt;For example, we’ve all been in a situation where a disagreement arises, but we &lt;em&gt;know&lt;/em&gt; we’re right. How can we lead the other person to reach our conclusion? From experience, you probably know that fighting and arguing usually leads to nowhere, but too often, we let our emotions get the best of us. We don’t always do what’s rational or expected, which is part of what makes us human! Going back to the question, however, how can we guide people to our point of view?&lt;/p&gt;

&lt;p&gt;Currently, I’m reading the infamous book &lt;em&gt;How to Win Friends and Influence People&lt;/em&gt; by Dale Carnegie (which was written back in 1936) and it brings up fair points on this exact issue. To be honest, I do feel the wording of the title could be changed, it already seems a little ingenuine! Anyways, Carnegie delineates that we of course want to avoid arguing at all costs. In the case there is a disagreement, however, he mentions to stay calm, listen to the other person, and ask questions (kind of like Socrates!). Additionally, it may help to start the conversation with a totally different topics, such as one the other person genuinely cares about, to “warm them up”.&lt;/p&gt;

&lt;p&gt;These techniques seem fine and dandy, but for me, they bring up the question of genuinity. Is it right to intentionally talk about something you &lt;strong&gt;know&lt;/strong&gt; this other person will like? To be the devil’s advocate here, it seems like the book is suggesting you bring up a topic that you may not be genuinely interested in, but you feign it to get that person to like you more and be more willing to listen to you. I definitely understand that if you can connect with someone personally, even if there is a disagreement, they will be more likely to listen to your ideas. However, is your intention with this simply to get what you want or to actually understand this other person and their interests? Perhaps I’m thinking too far into this.&lt;/p&gt;

&lt;p&gt;Another section of the book mentions how if you know you’re in the wrong, to bring it up before the other person does and in a way that they can feel like they’re showing you mercy. For example, if you get pulled over for speeding, instead of trying to fight the cop or giving excuses on why you were speeding, Carnegie suggests that if you admit to what you did and almost in a pleading manner, the police is more likely to be lenient. This is because of people’s desire to have their ego boosted. If you fought the cop on this, denying that you should get ticketed, the cop may feel like his/her authority is being tested and fight back. Everyone wants to feel important and by insisting to give you a ticket you don’t want, the cop can feel important. On the other hand, if you admit you’re wrong, the cop is unable to argue with that and in order to feel important, he/she may show mercy and let you go. (Obviously, this is not saying that if you get pulled over and try this tactic it will work, it’s just an example very similar to one Carnegie outlines in the book.)&lt;/p&gt;

&lt;p&gt;Again to me, it seems a little excessive and almost manipulative to intentionally grovel just because you think you can get out of a ticket or a consequence. I agree with admitting when you’re wrong, but I believe there are better ways to do it… This to me just seems like the other end of the spectrum than arguing incessantly and allowing your temper to take over, and I personally think I’d feel more comfortable with an approach in the middle: staying calm and admitting if you’re wrong but not to the point of groveling.&lt;/p&gt;

&lt;p&gt;For me in all of this, I think it goes back to the intentions. If your intentions are to avoid conflict, I think actions similar to the two outlined above may be okay. But if your intentions are solely to get exactly what you want and you implement the techniques above, that’s where the line blurs a little.  But then again, can you always tell what someone’s intentions are? I myself am still trying to develop my own stance on this topic.&lt;/p&gt;

&lt;p&gt;Anyways, I recommend this book! &lt;em&gt;How to Win Friend and Influence People&lt;/em&gt; by Dale Carnegie. I think there are insightful things and great personal examples in the book, but I also think simply following his steps are not enough and as a reader, you should think about each one pretty carefully. They’ve got to be internalized for each person and there has to be genuine concern and care in the actions (that’s the tough part!)&lt;/p&gt;

&lt;p&gt;Just some topics to reflect on:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How do you tend to resolve conflict and settle disagreements? Do they normally end positively?&lt;/li&gt;
  &lt;li&gt;How can you be genuine about your actions/intent in every day situations?&lt;/li&gt;
  &lt;li&gt;Do you have a technique that helps you work well with people? I’d love to hear about it!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Best wishes,&lt;/p&gt;

&lt;p&gt;Al&lt;/p&gt;</content><author><name></name></author><summary type="html">This topic has been something that really interested me this summer. I talked to my mom a lot about it and from her, realized that working effectively with people is probably one of the hardest things we have to learn to do. It’s not something you can only read from a textbook and understand immediately; it takes many experiences and practice to learn how to work with people without offending or hurting anyone.</summary></entry><entry><title type="html">Banana Bread with Homemade Oat Flour</title><link href="http://localhost:4000/food_for_thought/banana_bread/" rel="alternate" type="text/html" title="Banana Bread with Homemade Oat Flour" /><published>2019-09-10T06:25:40-07:00</published><updated>2019-09-10T06:25:40-07:00</updated><id>http://localhost:4000/food_for_thought/banana_bread</id><content type="html" xml:base="http://localhost:4000/food_for_thought/banana_bread/">&lt;p&gt;Prep Time: 15 minutes&lt;/p&gt;

&lt;p&gt;Cook Time: 45 minutes - 1 hour&lt;/p&gt;

&lt;p&gt;Serves: a LOT (about 1 loaf of bread)&lt;/p&gt;

&lt;p&gt;Difficulty: Coconut Mall from Mario Kart&lt;/p&gt;

&lt;h3 id=&quot;ingredients&quot;&gt;Ingredients&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Remember, these are all relative ish! Adapt as you wish!&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1/2 cup extra virgin olive oil + extra for greasing the pan&lt;/li&gt;
  &lt;li&gt;2/3 cup &lt;strong&gt;loose&lt;/strong&gt; brown sugar&lt;/li&gt;
  &lt;li&gt;1 tsp vanilla&lt;/li&gt;
  &lt;li&gt;2 eggs&lt;/li&gt;
  &lt;li&gt;3 mashed ripe bananas&lt;/li&gt;
  &lt;li&gt;1/4 cup milk (or any milk substitute, or even water)&lt;/li&gt;
  &lt;li&gt;2.5 cups oats&lt;/li&gt;
  &lt;li&gt;1/2 tsp salt&lt;/li&gt;
  &lt;li&gt;1/2 tsp baking soda&lt;/li&gt;
  &lt;li&gt;Add ins; my favorites are walnut and dark chocolate chips! But any type of nut, dried fruit, shredded coconut, etc will be fantastic&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;instructions&quot;&gt;Instructions&lt;/h3&gt;

&lt;h4 id=&quot;making-oat-flour&quot;&gt;Making Oat Flour&lt;/h4&gt;

&lt;p&gt;As a general rule of thumb, oat flour can substitute regular flour! Just be careful because the oats tend to absorb liquid a lot quicker than all purpose flour, so some modifications may have to be made. If a recipe asks for 1 cup of all purpose flour, you can blend 1 1/4 cups of oats to make approximately 1 cup of oat flour.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pour your oats in a blender or food processor (I personally prefer a blender!)&lt;/li&gt;
  &lt;li&gt;Start on a low speed and gradually bring your blender to high speed&lt;/li&gt;
  &lt;li&gt;Blend/Process oats until it is a fine flour (approximately 30 seconds)&lt;/li&gt;
  &lt;li&gt;Shake it around a bit to make sure there are no large chunks of oats that got left behind&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;rest-of-instructions&quot;&gt;Rest of Instructions&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Preheat the oven to 350F&lt;/li&gt;
  &lt;li&gt;Using an electric beater (or whisk and a really strong arm), cream together the oil and brown sugar&lt;/li&gt;
  &lt;li&gt;Add vanilla and eggs and beat until consistent&lt;/li&gt;
  &lt;li&gt;In a separate bowl, combine the dry ingredients&lt;/li&gt;
  &lt;li&gt;Alternate adding the dry ingredients with adding the mashed bananas and milk into the liquids. You can use a rubber spatula to mix at this point&lt;/li&gt;
  &lt;li&gt;Grease your breadpan using just a little bit of olive oil (just enough to cover the inside surfaces)&lt;/li&gt;
  &lt;li&gt;Pour your batter and place in the oven for around 45 minutes to an hour. I’ll generally set an initial timer to 35 minutes and check on it periodically.
    &lt;ul&gt;
      &lt;li&gt;Note: if you’re using a pan with a wider base, I would check on it more often.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Let it cool for around 20 minutes, and if you aren’t serving it right away, cover with alumnium foil to avoid the bread from drying out.&lt;/li&gt;
  &lt;li&gt;Enjoy! :D&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;so-why-oat-flour&quot;&gt;So Why Oat Flour?&lt;/h4&gt;

&lt;p&gt;The reason I prefer to use oat flour boils down to how the grain is processed to make regular flour. The grain in all purpose flour is processed in a way that it loses most of its nutrients and fiber. Oats, on the other hand, are considered a whole grain which means it goes through less processing and holds onto most of its fiber and nutrients. This not only provides important nutrients to your body, but also can help keep you full for longer! Whole wheat flour has similar properties to making oat flour, but it tends to be a little more expensive. Using just a blender or food processor, you can ensure you’re giving your body the nutrients you need without spending too much!&lt;/p&gt;</content><author><name></name></author><summary type="html">Prep Time: 15 minutes</summary></entry><entry><title type="html">Eat to Beat Disease: a MUST READ</title><link href="http://localhost:4000/food_for_thought/eat-to-beat-disease/" rel="alternate" type="text/html" title="Eat to Beat Disease: a MUST READ" /><published>2019-09-05T06:25:40-07:00</published><updated>2019-09-05T06:25:40-07:00</updated><id>http://localhost:4000/food_for_thought/eat-to-beat-disease</id><content type="html" xml:base="http://localhost:4000/food_for_thought/eat-to-beat-disease/">&lt;p&gt;The other day I was at the library, looking for a quick read before I head off to school in a week, and I spotted a thick book with color foods on the front titled &lt;em&gt;Eat to Beat Disease&lt;/em&gt;. ‘This sounds interesting!’ I thought to myself, and as I was in a hurry to leave, I quickly checked it out. That night, I started reading after dinner and literally could not stop. This book is everything I’ve been looking for! And I’m not exaggerating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/food/eat_to_beat.jpg&quot; alt=&quot;Selfie with my favorite new book&quot; class=&quot;images third&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;caption&quot;&gt;Here it is! My favorite new read&lt;/p&gt;

&lt;p&gt;For the longest time, I wanted a resource (a collection of journal articles, podcasts, book, &lt;em&gt;anything!&lt;/em&gt;) that explained why some foods are healthy and others aren’t with scientific backing, and this book &lt;span class=&quot;standOut&quot;&gt;does it all&lt;/span&gt;. It explains why certain foods are good for you beyond the scope of how full it makes you or whether the calories are “worth it”. In fact, I’m currently about halfway through the book, and the author, Dr. William Li, has barely even talked about calories! WAKE UP AMERICA! Calories are &lt;strong&gt;not&lt;/strong&gt; the only measure of health!&lt;/p&gt;

&lt;p&gt;As a quick sneak peek, Dr. Li first explains five of the body’s amazing defense mechanisms against diseases including cancer, diabetes, heart disease, and many others that seem to only be treatable by modern medicine. The five mechanisms are summarized as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Angiogenesis: building blood vessels to various parts of the body&lt;/li&gt;
  &lt;li&gt;Regeneration: using stem cells to repair injuries&lt;/li&gt;
  &lt;li&gt;Our gut’s microbiome: the good bacteria living inside of us&lt;/li&gt;
  &lt;li&gt;DNA Projection and Repair: affecting expression of certain genes&lt;/li&gt;
  &lt;li&gt;Immunity: using our immune system to build resistance against certain diseases/invaders.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Except, read the book for a much more thorough and in depth explanation of the mechanisms; this is barely the tip of the iceberg! Then, Dr. Li goes on to explain which foods affect each of the five mechanisms and guess what! They’re mostly foods that you’re probably already eating! He backs up each food listed with scientific evidence, whether from clinical trials or experiments done in a lab setting, and usually uses more than 1 experiment as an example.&lt;/p&gt;

&lt;p&gt;For me personally, before picking up this book, I kind of judged foods on if they were healthy or not based on if they had a lot of fiber and would fill me up while minimizing the number of calories I consumed. However, this is such a black and white scale for a field as colorful as a rainbow, almost literally! There’s so much more to health than just digestion rate. For example, it’s known that cheese is pretty calorie/energy dense and contains quite a bit of saturated fat, so I tended to avoid it as much as I can. However, cheese (especially European cheese) has a lot of probiotics and prebiotics (good bacteria and food for the bacteria in our microbiome, respectively) that will improve our overall health! In moderation, of course, cheese is really beneficial!&lt;/p&gt;

&lt;p&gt;I’m already looking at the foods I eat and making a mental note of which defense mechanism I’m improving by eating this, but the list is so long, I’ll definitely have to give &lt;em&gt;Eat to Beat Disease&lt;/em&gt; another read to understand more!&lt;/p&gt;

&lt;p&gt;As summer turns into Fall and we’re stuck inside anyways, &lt;strong&gt;definitely&lt;/strong&gt; put this on your read list!&lt;/p&gt;

&lt;p&gt;Note: it is a little scientifically dense, and while Dr. Li does a great job of explaining it, if a concept is new, don’t be afraid to re-read a part a few times or use Google! I know I definitely did :)&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">The other day I was at the library, looking for a quick read before I head off to school in a week, and I spotted a thick book with color foods on the front titled Eat to Beat Disease. ‘This sounds interesting!’ I thought to myself, and as I was in a hurry to leave, I quickly checked it out. That night, I started reading after dinner and literally could not stop. This book is everything I’ve been looking for! And I’m not exaggerating.</summary></entry><entry><title type="html">Sure about sugar?</title><link href="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar.html" rel="alternate" type="text/html" title="Sure about sugar?" /><published>2019-08-27T00:00:00-07:00</published><updated>2019-08-27T00:00:00-07:00</updated><id>http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar</id><content type="html" xml:base="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/27/sugar.html">&lt;h5 id=&quot;all-sugar-is-bad-right&quot;&gt;All sugar is bad right?&lt;/h5&gt;

&lt;p&gt;Okay that’s a tough question. Sugar by itself is not necessarily bad; it’s a good source of quick energy for our bodies. &lt;em&gt;HOWEVER&lt;/em&gt;, too much sugar, especially in its most processed form can be pretty detrimental to our bodies and lead to weight gain as well as medical issues.&lt;/p&gt;

&lt;h5 id=&quot;how-does-our-body-digest-sugar&quot;&gt;How does our body digest sugar?&lt;/h5&gt;

&lt;p&gt;Basically, sugar will be broken down to glucose, which is the form of energy our cells require, and the glucose gets sent to one of three places:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Our cells, to give them energy&lt;/li&gt;
  &lt;li&gt;Our liver, to store glucose for later in the form of glycogen (which you don’t need to worry about)&lt;/li&gt;
  &lt;li&gt;Our adipose tissue as fat.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our cells prefer to have sugar because they can quickly be broken down into glucose, but our cells can only take so much glucose at a time. Same with our liver. The first two have a capacity of how much glucose gets sent there, but adipose tissue &lt;span class=&quot;standOut&quot;&gt; can take as much glucose as it wants&lt;/span&gt;. That’s where the dangerous part is.&lt;/p&gt;

&lt;p&gt;If our body breaks down too much sugar at once and has too much glucose for our cells and liver, it will send all the excess glucose to turn into fat, which will lead to weight gain.&lt;/p&gt;

&lt;h5 id=&quot;so-we-should-avoid-all-sugar&quot;&gt;So we should avoid all sugar?&lt;/h5&gt;

&lt;p&gt;Not necessarily! The main difference in sugar that is, for example, added in candy and sodas, and sugars in whole fruit is the magical thing called &lt;span class=&quot;standOut&quot;&gt; fiber&lt;/span&gt;. The sugars in fruit are all encased by a lot of fiber, which if you recall, slows down the digestion of food. This means that the sugar in fruit gets slowly broken down and transported to the appropriate location over a longer period of time. If you ate a candy bar, on the other hand, basically all that sugar gets broken down to glucose and taken to the appropriate destination all at once. Sugars in fruit are able to provide your &lt;em&gt;cells&lt;/em&gt; with a more constant form of energy, which also helps you feel fuller and maintain energy.&lt;/p&gt;

&lt;p&gt;In fact, fiber is actually the main difference between white sugar and brown sugar. Although brown sugar is not the same as the sugar we get in whole fruits, it has more fiber compared to white sugar, and thus is marketed as healthier.&lt;/p&gt;

&lt;h5 id=&quot;how-do-we-know-what-has-added-sugar&quot;&gt;How do we know what has added sugar?&lt;/h5&gt;

&lt;p&gt;Ah yes this can be the tricky one. The first place I would check is the nutrition label. Under ‘sugar’, some labels have a row for ‘added sugar’ which is not very desirable. However, for a more thorough check, you must go to the ingredient list. &lt;span class=&quot;standOut&quot;&gt;But sugar is tricky.&lt;/span&gt; After people realized sugar was leading to weight gain and other health complications, many companies became sly about hiding sugar into their products by using different names for sugar but in reality, it just means ‘Added Sugar’. Some common ones to look out for are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The mono and disaccharides (dextrose, fructose, galactose, glucose, lactose, maltose, and sucrose)&lt;/li&gt;
  &lt;li&gt;Cane sugar, evaporated cane juice&lt;/li&gt;
  &lt;li&gt;High Fructose Corn Syrup&lt;/li&gt;
  &lt;li&gt;Corn syrup solids&lt;/li&gt;
  &lt;li&gt;Dextrin&lt;/li&gt;
  &lt;li&gt;Glucose syrup solids&lt;/li&gt;
  &lt;li&gt;Maltodextrin&lt;/li&gt;
  &lt;li&gt;Agave Nectar/Syrup (very similar to honey)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;im-never-eating-sweets-again-right&quot;&gt;I’m never eating sweets again, right?&lt;/h5&gt;

&lt;p&gt;I’d say enjoy in moderation! It’s very difficult to cut out something entirely from your diet because you’ll probably start to crave it even more, causing a binge eating session. It’s okay to enjoy that sweetness of a candy bar or ice cream once in a while, but be mindful of what’s happening inside your body after you eat it.&lt;/p&gt;</content><author><name></name></author><category term="nutrition" /><summary type="html">All sugar is bad right?</summary></entry><entry><title type="html">Week 9: Wrapping Things Up!</title><link href="http://localhost:4000/food_for_thought/FM-week-9/" rel="alternate" type="text/html" title="Week 9: Wrapping Things Up!" /><published>2019-08-26T18:31:00-07:00</published><updated>2019-08-26T18:31:00-07:00</updated><id>http://localhost:4000/food_for_thought/week-9</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-9/">&lt;h3 id=&quot;monday-august-26-2019&quot;&gt;Monday August 26, 2019&lt;/h3&gt;

&lt;p&gt;Happy Monday! This past weekend was so relaxing! It was kind of hard to get up for work today haha. And I faced a bit of a surprise when I arrived at the museum because my badge didn’t work anymore! I was supposed to end the internship last week, but we extended it for one week and we didn’t let Access Control know… oopsies&lt;/p&gt;

&lt;p&gt;It’s all fixed now, don’t worry :D&lt;/p&gt;

&lt;p&gt;After that mini roadblock, I continued to examine the models and make changes to improve them. In my model training to find the differences between Lycopodium and Selaginella, some folds did really well and training accuracy reached 90% while validation accuracy lagged behind, but in other folds, the training &lt;em&gt;and&lt;/em&gt; validation accuracy both plummeted to around 50% by the 10-15 epoch and just couldn’t recoveer. In all epochs, however, loss was constantly decreasing. I’m not really sure what’s happening there and why it would happen on some folds and not others… maybe there’s a few images that are just really throwing the model off? Although I did look through them manually before. Perhaps it’s worth another look.&lt;/p&gt;

&lt;p&gt;I first looked into doing image augmentation, but I originally thought that it would result in the model training on &lt;em&gt;more&lt;/em&gt; images than you have every epoch. Turns out, it just trains on a smaller sample, but each epoch it uses &lt;em&gt;randomly altered&lt;/em&gt; images that are different to the model. It may be useful in another application (like the frullania), but currently when we have so many images already, I don’t think we need it.&lt;/p&gt;

&lt;p&gt;I tried playing around with the regularizers today in the Dense and Convolutional-2D (Conv2D) layers, but most combinations I tried resulted in the model overfitting very early and the training and validation accuracy would begin to decrease by the 6-7th epoch. (To see what changes I made, check out &lt;a href=&quot;https://docs.google.com/spreadsheets/d/15972K_rdv3zpnvnV--wSaTpiK0jnVDkBzhUWjre5BLg/edit?usp=sharing&amp;quot;&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;this doc&lt;/a&gt;. The model started to learn again pretty well when I kept activity regularizers in the Dense layers, despite there not being an exact equivalent in the Smithsonian Methematica Model.&lt;/p&gt;

&lt;p&gt;We’ll see what happens tomorrow morning!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;p&gt;PS, Matt wants me to try running the model again on the two frullania species again but the images being from farther away. I want to look into bootstrapping for this and hopefully synthetically expand our data set.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-28-2019&quot;&gt;Tuesday, August 28, 2019&lt;/h3&gt;

&lt;p&gt;First thing this morning, I checked on the training of the model on Lycopodium and Selaginella. For the most part, it was okay; there was still a bit of overfitting, but the trend of validation accuracy seemed to follow training accuracy, like in the graph below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/val_accuracy_1.png&quot; alt=&quot;Slightly overfit accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, on other folds, we got something disastrous like this:
&lt;img src=&quot;/assets/images/blog/fm/val_accuracy_8.png&quot; alt=&quot;Dropped accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m not entirely sure why on most folds, the model seems to be training well, while on others, it’s completely unable to after a certain amount of time. I don’t think it’s overfitting because even the training accuracy decreased. Well, tomorrow, I’m meeting with Dr. Iacobelli, the computer science professor at Northeastern Illinois University, so hopefully he’ll be able to offer more insight.&lt;/p&gt;

&lt;p&gt;Additionally, I received new images of the frullania species today. These ones are more zoomed in and hopefully will allow the model to pick up on more details in the differences between the two plants. I implemented some bootstrapping to train the model because we only had a little less than 300 images which I’m not sure is enough to be able to completely train the model on. I left it going overnight, so I’ll be able to check in on Thursday.&lt;/p&gt;

&lt;p&gt;At this point, most of my big machine learning changes are wrapping up and I’ve been working on documentation, documentation, and documentation! Especially since I was pretty much the only one working on this project this summer, it’s &lt;strong&gt;extremely&lt;/strong&gt; important that someone could study my work for a bit and pick up where I left off.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-28-2019&quot;&gt;Wednesday August 28, 2019&lt;/h3&gt;

&lt;p&gt;This morning I met with Dr. Iacobelli, Beth, and Dr. Campbell (unfortunately Matt coudln’t make it) We updated Dr. Iacobelli and here are his responses/advice. Regarding the random dropped accuracy I wrote about yesterday, he recommends that we meticulously check the code first. There may a bug (such as divide by 0) that’s throwing everything off. Additionally, it simply may be lack of processing power in the computer causing a data overflow.&lt;/p&gt;

&lt;p&gt;Another thing he noticed is that the images we’re using from the Field Musuem are &lt;em&gt;not&lt;/em&gt; consistent in the herbarium sheet layout. Some have labels in the upper right corner, some are lower left, some have specimen in the middle, some have specimen in the lower right corner. The inconsistencies in layout may be providing too much noise for the model, so he recommends that we try a smaller sample of images that have a more consistent layout an see if there is an improvement in performance.&lt;/p&gt;

&lt;p&gt;Additionally, there is a computer at NEIU with 64 GB RAM that Beth is going to be trying our model with the Smithsonian images (which I didn’t know we had!) and we found out there is a computer at the museum with &lt;span class=&quot;standOut&quot;&gt; one terabyte of RAM. I repeat, of RAM. &lt;/span&gt; I almost fainted when I heard that.&lt;/p&gt;

&lt;p&gt;Although I’m wrapping up this week, I’m excited for Beth to be able to try things out!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-29-2019&quot;&gt;Thursday August 29, 2019&lt;/h3&gt;

&lt;p&gt;When I came in this morning, the model that was training on images of the Adiantum and Blechnum genera finished training, so I created a script to test it. Since the model really consists of 10 trained models (given that we used 10 fold cross validation), I had to take that into account. One way to do this would be see which prediction output each model gave and the final prediction would be what the majority of the models output. However, this poses two possible issues:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What if 5 models say class A and 5 say class B? There wouldn’t be an odd number in this case to be the tie breaker&lt;/li&gt;
  &lt;li&gt;If one model is 99% sure that an image is class A but another model is only 50% sure that it’s class A, using the method described above would give each of these equal weights when it maybe shouldn’t.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To overcome this, I decided to take a step back and look at the probability outcomes that are used to determine each model’s final answer. Basically, the final layer of the model outputs two numbers: the probability that an image is one class or another. So what I did was I kept track of each of these probabilities and for each image, added up all the models’ class A probability and class B probability separately. The final answer would be whichever summed probability is higher. This way, the “confidence” of each of the 10 models is taken into account.&lt;/p&gt;

&lt;p&gt;Honestly, that didn’t take me very long today, so when I tested the Adiantum/Blechnum model, we got a whooping &lt;span class=&quot;standOut&quot;&gt;97.8% accuracy!!&lt;/span&gt; It was tested on 400 new images and this is the best I’ve ever gotten :)))&lt;/p&gt;

&lt;p&gt;This is good news because it means the model can work! We just need to tweak values and perhaps look at cleaning our input images for Lycopodium and Selaginella.&lt;/p&gt;

&lt;p&gt;The rest of the day was mainly spent documenting my work. I explained to Matt how my Google Drive was organized and how to use Github as well as documented within the Drive where everything is.&lt;/p&gt;

&lt;p&gt;I did, however, forget to upload my final cross-validation testing file to Github, which is what I’ll do first thing tomorrow morning.&lt;/p&gt;

&lt;p&gt;Last day tomorrow! What a melancholy feeling…&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-30-2019&quot;&gt;Friday August 30, 2019&lt;/h3&gt;

&lt;p&gt;Today was spent mainly double checking my documentation on Github and Google Drive. I called Beth and we went over each file in Github and she seems confident on where I’m leaving off, which is great! I’m so excited for her to continue this project and be able to run the model on a computer that has enough processing power.&lt;/p&gt;

&lt;p&gt;Furthermore, Matt worked on getting the images contrasted a bit more yesterday; now the background of the images are very white compared to the specimen in focus and it helped get rid of some noise. We only have a little less than 150 of either group, but we ran the model for 50 epochs and we actually got some results! Although overfitting still occurred, the validation accuracy definitely grew as the number of epochs increased! This is a great sign! It means the machine is able to generalize what it’s learning on the training data onto new images.&lt;/p&gt;

&lt;p&gt;Since we didn’t run for very long and we don’t have many images, we were able to start another run before the end of the day. Matt went through the images and deleted any that were actually the wrong specimen or weren’t sterile as this type of noise could be hurting the model’s performance. Then, before I left, I ran two models: one with the same exact specs as the last run, just with the new images, and one with the new images &lt;em&gt;and&lt;/em&gt; more epochs. Although I won’t be able to see the results, Matt will be able to check it out and report back!&lt;/p&gt;

&lt;p&gt;And that’s a wrap :) although I’m leaving, I know this project is going to go far in the hands of Beth, Matt, and Dr. Iacobelli. I’m excited to see where they are able to take this application of computer science into botany and am truly grateful for this opportunity!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 26, 2019</summary></entry><entry><title type="html">Becoming Keen on Quinoa</title><link href="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa.html" rel="alternate" type="text/html" title="Becoming Keen on Quinoa" /><published>2019-08-20T00:00:00-07:00</published><updated>2019-08-20T00:00:00-07:00</updated><id>http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa</id><content type="html" xml:base="http://localhost:4000/food_for_thought/food_in_food_for_thought/nutrition/2019/08/20/quinoa.html">&lt;h5 id=&quot;sooo-what-is-quinoa&quot;&gt;Sooo what is quinoa?&lt;/h5&gt;

&lt;p&gt;Quinoa (pronounced keen-wah) is a type of grain that is in my opinion both delicious and extremely nutritious. Uncooked, they look like little seeds, normally yellow or brown, but after boiling, they become soft and fluffy. In addition to being an excellent source of carbs and energy, they have a surprising amount of protein and are even&lt;span class=&quot;standOut&quot;&gt; a complete protein&lt;/span&gt;! Meaning quinoa contains &lt;strong&gt;all nine&lt;/strong&gt; essential amino acids.&lt;/p&gt;

&lt;h5 id=&quot;but-why-is-it-better-for-me-than-flour-or-rice&quot;&gt;But why is it better for me than flour or rice?&lt;/h5&gt;

&lt;p&gt;For a few reasons, actually. First quinoa is not as processed as flour and thus retains more of its vitamin and minerals as well as fiber. Compared to rice, quinoa’s glycemic index is significantly lower (53 compared to 73 of white rice) which means that it does not produce as big of an insulin spike. (Which is good! We don’t want to unnecessarily tire out our pancreas.) The main reason for this is &lt;span class=&quot;standOut&quot;&gt; fiber&lt;/span&gt;!!&lt;/p&gt;

&lt;p&gt;Recall that fiber is essentially a food’s “protective barrier” to being digested. But of course, our digestive system always overcomes it. Fiber itself is undigestable and slows down our body’s ability to digest the food it encases. This may sounds bad, but it’s actually extremely good, especially for individuals who are wary of caloric intake. The fiber helps us eat the same number of calories, but feel fuller for longer, reducing the cravings we often feel after eating pasta or other refined grains.&lt;/p&gt;

&lt;p&gt;If you take away anything from this blog post, I hope it’s that &lt;span class=&quot;standOut&quot;&gt; quinoa = fiber! &lt;/span&gt;&lt;/p&gt;

&lt;h5 id=&quot;what-do-you-even-eat-it-with&quot;&gt;What do you even eat it with?&lt;/h5&gt;

&lt;p&gt;I love quinoa as a rice/grain substitute! Maybe it’s the Asian side in me, but I’ll often have a bowl of quinoa with my dinner or lunch just instead of rice. Additionally, it’s really good in salads and or various types of bowls. It’s really quick and easy to cook a bunch of quinoa at once and refrigerate; it can probably last a good week or so!&lt;/p&gt;

&lt;h5 id=&quot;im-a-broke-college-student-how-can-i-afford-this&quot;&gt;I’m a broke college student, how can I afford this?&lt;/h5&gt;

&lt;p&gt;Admittedly, quinoa can be more expensive than some easier foods, such as pasta or rice. But remember, nutrition is about a long run investment. Although you may be spending a bit more now, you’re treating your body better (which in itself is priceless!) and maybe even saving money on future medical bills. Plus, a small bag of quinoa actually makes quite a lot, I think you’d be surprised :)&lt;/p&gt;

&lt;h5 id=&quot;how-do-you-cook-quinoa&quot;&gt;How do you cook quinoa?&lt;/h5&gt;

&lt;p&gt;Easy! Simply put about 1 cup of quinoa and 1 cup of water in a pot and bring to a boil. If you’re cooking less, I would put a little more water, about a 1:1.25 quinoa to water ratio. Once it starts to boil, turn down the heat and cover the pot, allowing the quinoa to simmer. Once most of the water is gone, I would check on it frequently and stir to prevent any quinoa from sticking to the bottom.&lt;/p&gt;

&lt;p&gt;The quinoa is cooked if it looks almost “fluffy” and it shouldn’t be crunchy anymore.&lt;/p&gt;</content><author><name></name></author><category term="nutrition" /><summary type="html">Sooo what is quinoa?</summary></entry><entry><title type="html">Week 8: Implementing Cross Validation</title><link href="http://localhost:4000/food_for_thought/FM-week-8/" rel="alternate" type="text/html" title="Week 8: Implementing Cross Validation" /><published>2019-08-19T06:25:40-07:00</published><updated>2019-08-19T06:25:40-07:00</updated><id>http://localhost:4000/food_for_thought/week-8</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-8/">&lt;h3 id=&quot;monday-august-19-2019&quot;&gt;Monday August 19, 2019&lt;/h3&gt;

&lt;p&gt;This morning, Beth came to the Field Museum to work with me on implementing stratified cross validation and ROC testing. Stratified cross validation essentially partitions the data into groups of approximately equal distributions of each class. ROC (stand for Receiving Operator Curve) is a type of graph that can help us quantify how “good” our model is. It uses the percentage of true positives and false positives and the closer the curve hugs the y-axis, the more confident it is. Take a look at the image below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/ROC_curve.png&quot; alt=&quot;ROC Curve&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The straight line represents if it were just up to chance, so if the ROC curve is close to that, it’s bad.&lt;/p&gt;

&lt;p&gt;I got the code to run with stratified cross validation and creating an ROC curve at the end with small amounts of bogus test data, but when I tried to import larger amounts of useful data, the program seems to be crashing and I’m not sure why. I ran the old model again (prior to implementing cross validation) and it seemed to work okay, but in this new script, it feels like everything’s falling apart (rip). Before the program started crashing, it would train but ridiculously overfit on the training data while validation accuracy didn’t increase at all :(&lt;/p&gt;

&lt;p&gt;I’m trying to see if I accidentally changed something when adapting the model, but then the code crashed entirely, freezing the computer.&lt;/p&gt;

&lt;p&gt;This is definitely a rough note to end the day on, but I’m going to try not to let it bother me too much and come in tomorrow ready to problem solve!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-20-2019&quot;&gt;Tuesday August 20, 2019&lt;/h3&gt;

&lt;p&gt;This morning, I was able to fix why the code would crash my computer! When I was using the split function on the stratifiedKFold object and accessing features and labels that made up the training and validation sets, they had to be numpy arrays. My labels data structure was only a list and thus would throw an error and crash it. After making a few more tweaks on random seeds and epochs, aroudn 10:30 AM I started running the 10 fold cross validation and it hasn’t stopped since. I’ve left it on and hope no one messes with it! Tomorrow morning, we’ll be able to check out results and see what changes we can make to improve the model.&lt;/p&gt;

&lt;p&gt;While the model was training, I was tasked with a side task. There’s a bryophyte checklist of the Java region that’s written as a word document. We want to parse the document and store the genus, species, and founder into a CSV. Currently, I’m using a package called python-docx and basic python string manipulation to parse out the desired information. The only downside is this code I’m writing is only specific to this document which of course isn’t as fun in CS.&lt;/p&gt;

&lt;p&gt;Have a great day!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-21-2019&quot;&gt;Wednesday August 21, 2019&lt;/h3&gt;

&lt;p&gt;Today was a &lt;strong&gt;long&lt;/strong&gt; day, oof. To start out, Matt and I talked about the project and thought about why our results weren’t exactly matching that of the Smithsonian. First, and foremost, it’s most likely because of our data that we’re using. The Smithsonian is using images from their own herbarium while we’re using images from the Field Museum herbarium and obviously those will produce different results. Within the family of Lycopodiaceae, the distribution of images across genera are different between their herbarium and ours. Additionally, they were able to use a lot more images due to processing power. I found out today, that using around 4000 images is a lot for the computers we have (i5 Intel Cores with 8 GB RAM). Sometimes the computer just freezes entirely and you have to hard restart it. Thus, I decreased my number of images to around 3300 and it seems to be running okay!&lt;/p&gt;

&lt;p&gt;Additionally, instead of comparing families of plants (Lycopodiaceae and Selaginellaceae), we decided to simplify the inputs to two genera of plants (Lycopodium and Selaginella). Selaginella is the only genus in the family Selaginellaceae (as a reminder the hierarchy goes family &amp;gt; genus &amp;gt; species) and Lycopodium is one of around 8 or so genera in the family Lycopodiaceae. Hopefully this will provide less noise in the first class and make it easier for the machine to detect prominent differences.&lt;/p&gt;

&lt;p&gt;Additionally, just for kicks, Matt wanted me to run the model to compare two new and different genera: adiantum and blechnum. This is because these two families look VERY different to the eye, so the model should do a relatively good job with this. I spent most of today preparing the images to be run on a different computer and started running the model before I left.&lt;/p&gt;

&lt;p&gt;I got in around 8:15 this morning and didn’t leave till around 5:30, I’m going to go sleep a lot now!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-22-2019&quot;&gt;Thursday August 22, 2019&lt;/h3&gt;

&lt;p&gt;I came in this morning and good news, nothing crashed! The models are still chugging along doing their own thing. Something I noticed and continue to notice is that while training accuracy will rise somewhat consistently with each epoch, validation accuracy fluctuates a LOT more, like in the graph below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/8.14.19_12.30.png&quot; alt=&quot;Fluctuating Validation Accuracy&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m going to look into this more, but after a quick Google search, it seems one issue may be too much dropout. So after this model is finished training, I’m going to look into making that change (which might end up having to be done tomorrow)!&lt;/p&gt;

&lt;p&gt;The program I started running yesterday with Lycopodium and Selaginella was taking forever… after 18 hours, it only completed 5 folds of the 10 fold cross validation and Beth and I decided that it was too long to realistically experiment. Thus, we decreased the images from 256 x 256 pixels down to 128 x 128 and it seems to be going much faster! Each epoch now takes a little over a minute instead of around 10 minutes.&lt;/p&gt;

&lt;p&gt;Additionally, I’m running a baseline test with the model and images of Frullania rostrata and Frullania coastal. As a reminder, these are two possibly distinct species that we want to gather further evidence that they are different. Although the model isn’t “tuned” to picking up the differences in the Frullania, it’ll be nice to have a baseline essentially and see how to go from there. Beth had also mentioned that there’s a way to see what features each layer of the model is picking up, which would be helpful in our case to see what layers are working for our purpose and what should be changed.&lt;/p&gt;

&lt;p&gt;Take it easy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;h3 id=&quot;friday-august-23-2019&quot;&gt;Friday August 23, 2019&lt;/h3&gt;

&lt;p&gt;Happy Friday! When I came in this morning, all three of my models finished running and here’s a brief summary of the results:&lt;/p&gt;

&lt;p&gt;The Lycopodium/Selaginella results: The validation accuracy followed the training accuracy, but there was still a lot of fluctuation. In my next training, I increased batch size from 32 to 64 and increased epochs to 100. May as well since we have the weekend to run it. Before I left for the day, it finished training 2/10 folds and I noticed in the second fold, by the end neither training nor validation accuracy increased from 50%. This leads me to believe we shouldn’t increase batch size much above 32.&lt;/p&gt;

&lt;p&gt;The Adiantum/Blechnum results: This run was mostly as a baseline to see whichout changing the model, what results did we get. Again, they were pretty similar to before: validation accuracy seemed to follow training accuracy but with a LOT more and larger fluctuations. In this next training session, I decreased the initial dropout layer from 50% to 40% to see if perhaps less measures against overfitting would help.&lt;/p&gt;

&lt;p&gt;The Frullania (coastal/rostrata) results: awful. I really didn’t expect much but what we saw here in all the folds was a classic case of overfitting. The training accuracy kept increasing while validation accuracy remained fluctuating around 50%. To make this work, we would need to reconfigure the layers of the model to “detect” these certain features.&lt;/p&gt;

&lt;p&gt;On a more fun note, today our department had a large rpotluck! There were all sorts of food from Malaysian, Latin American, to Chinese! I brought carrots and hummus and a Chinese Eight Treasures Cake which got really good feedback! I ate literally so much food but it was absolutely amazing :)&lt;/p&gt;

&lt;p&gt;Have a great weekend in this beautiful weather!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 19, 2019</summary></entry><entry><title type="html">Week 7: Improving CNN Training</title><link href="http://localhost:4000/food_for_thought/FM-week-7/" rel="alternate" type="text/html" title="Week 7: Improving CNN Training" /><published>2019-08-12T06:25:40-07:00</published><updated>2019-08-12T06:25:40-07:00</updated><id>http://localhost:4000/food_for_thought/week-7</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-7/">&lt;h3 id=&quot;monday-august-12-2019&quot;&gt;Monday August 12, 2019&lt;/h3&gt;
&lt;p&gt;Today was essentially spent just making changes to the architecture/regularizers of my model and training it to hopefully get better results. The main issues I’m facing now is that validation accuracy fluctuates a lot. It’ll grow steadily for a little then drop down to almost 50% and grow a little more (such as in the image below).
&lt;img src=&quot;/assets/images/blog/fm/8.13.19_13.47.png&quot; alt=&quot;Accuracy Graph&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although it took all day because training the model currently takes around an hour, the main things I experimented with were changing the epsilon value (which prevents dividing by 0) and the regularizers. Regularizers essentially penalize larger weights, forcing the parameters to be small. I used L2 Regularization which adds a penalty term to the loss function that’s composed of the sum of the squares of the parameters.&lt;/p&gt;

&lt;p&gt;The validation accuracy is still fluctuating a lot but I just called it a day after working on this for a long time.&lt;/p&gt;

&lt;p&gt;See you tomorrow!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;tuesday-august-13-2019&quot;&gt;Tuesday August 13, 2019&lt;/h3&gt;
&lt;p&gt;Continuing the work of yesterday, I was a little frustrated with why the validation accuracy was so awful. Doing some more comparisons between my model architecture and the Smithsonian, I did notice a discrepancy. In the Smithsonian paper (which as a reminder used Mathematica, not Python) had layers called ‘linear layers’ which simply output weights*inputs+bias. I assumed a Dense layer in Keras did the same, but after doing some research, I found that I was wrong. I added the linear activation function to these layers and the results seemed to be at least a little more consistent in terms of validation accuracy/loss.&lt;/p&gt;

&lt;p&gt;Although it’s still far from perfect, the validation losses and accuracy seem to be following the training loss and accuracy a little more closely. I believe the fluctuations mean that the model is still overfitting to training data and is unable to be consistent with the validation cases. The next step I want to try is incorporating more drop out or possibly increasing regularizers. I do need to be careful with regularizers though; I don’t want to have too much and prevent the model from continuing to learn.&lt;/p&gt;

&lt;p&gt;In addition to using a model to learn between Selaginellaceae and Lycopodiaceae, we want to see if we can apply the same model architecture to two possibly different species of a plant under Frullania. This would be a completely new application and really cool! But still in the process of obtaining images right now. One thing that will be kind of difficult with that is we only have around 100 images of each while with the families I’m working with now, there are thousands and I’m still struggling to get a good model.&lt;/p&gt;

&lt;p&gt;Wish me luck, I’ll need it.&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-14-2019&quot;&gt;Wednesday August 14, 2019&lt;/h3&gt;

&lt;p&gt;This morning was a bit slow because training the model currently takes a very long time, but I added another dropout layer which seems to help with overfitting! Validation accuracy is fluctuating just a little bit less :) Additionally, I found that for some reason, when I end training and resume it again at the same epoch, it produces different results than if I don’t stop. This leads me to believe there is something that I haven’t been able to control in regards to the randomness of the model and it may be causing misleading results. This is something I’d like to look more into.&lt;/p&gt;

&lt;p&gt;Additionally, I received images today of two different species that Dr. Matt von Konrat (aka my boss) has been doing research on. They’re called frullania coastal and frullania rostrata (you may recall what they look like from previous posts about Morphosnake). Right now, we’re trying to build up the amount of evidence to show that these in fact are different species. However, we only have about 90-100 of each species, so I’m currently looking into using image augmentation with Keras to give ourselves more images and prevent overfitting. As a reference for myself, I’m using &lt;a href=&quot;https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/&quot; class=&quot;standOut&quot;&gt;this website&lt;/a&gt; as a reference. I can also look into changing my model to help against overfitting. For example, in the other project, my batch size was 32, but since I only have about 130 images total to train with right now, decreasing my batch size to 8 already improved validation accuracy without sacrificing too much time.&lt;/p&gt;

&lt;p&gt;This is something new and exciting to me! Hopefully we can get some results by early next week :)&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-july-15-2019&quot;&gt;Thursday July 15, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I worked on a data summary script to finalize a project that the high school interns worked on. As a reminder, their project was a community science activity on Zooniverse that allowed people to examine fern specimen. We wanted to see in general how close the community user responses were to an expert response (done by our very own Dr. Matt von Konrat). I worked on a script that took the exported data from Zooniverse and parsed through it. Although it wasn’t difficult, it took a while because Jessica (another intern!) and I kept chatting (oops!)&lt;/p&gt;

&lt;p&gt;After that, I started working back on the machine learning project. Last night, I called with Dr. Francisco Iacobelli, a computer science professor at Northeastern Illinois University. We talked about implementing k-fold Cross Validation methods to test the robustness of a model. The idea is that you take all your images and split it into 10 groups randomly. Choose one group to be the validation data and the successive group to be the testing data. The remaining data acts as training data. Train the model and save it, then repeat so all groups get a chance to be the testing data. In the end you have 10 trained models that really make up &lt;strong&gt;the model&lt;/strong&gt;. This makes a lot more sense to me after listening to Dr. Iacobelli talk about it; I had seen it online but for some reason it didn’t really stick until now.&lt;/p&gt;

&lt;p&gt;By the end of the day, I think I almost finished implementing the cross validation! I’m currently in the process of writing the loop that partitions the training, testing, and validation data, and after that, I just have to run and save the model for each group.&lt;/p&gt;

&lt;p&gt;I won’t be at the museum tomorrow, but I’ll try to get some work done on my own, mostly just documentation so I don’t have to do that later.&lt;/p&gt;

&lt;p&gt;Happy Thursday everyone!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-16-2019&quot;&gt;Friday August 16, 2019&lt;/h3&gt;

&lt;p&gt;Today I didn’t go to the museum, I had a meeting at Northeastern Illinois University with our grad student Beth, Dr. Campbell (a biology professor who’s been working with some of our summer interns), and Jose. We basically just updated everyone else where we are on the project and Beth and I communicated a bit more on how we can work together. She’s been pretty busy with other projects at the same time, so this was a good opportunity for us to touch base.&lt;/p&gt;

&lt;p&gt;After the meeting, it wasn’t worth it for me to go back to the Field Museum, so I went home and worked on doing some documentation. I updated the ReadMe file in &lt;a href=&quot;https://github.com/allisonchen23/ml_classifications&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;my Github for this machine learning project&lt;/a&gt;. Documentation is something I’ll really have to be on top of things about especially because not many people in this department are familiar with using python and machine learning.&lt;/p&gt;

&lt;p&gt;Although not terribly busy, today was a good day to prep me for the work I need to do next week!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 12, 2019 Today was essentially spent just making changes to the architecture/regularizers of my model and training it to hopefully get better results. The main issues I’m facing now is that validation accuracy fluctuates a lot. It’ll grow steadily for a little then drop down to almost 50% and grow a little more (such as in the image below).</summary></entry><entry><title type="html">Week 6: Continuing Machine Learning Project</title><link href="http://localhost:4000/food_for_thought/FM-week-6/" rel="alternate" type="text/html" title="Week 6: Continuing Machine Learning Project" /><published>2019-08-05T06:25:40-07:00</published><updated>2019-08-05T06:25:40-07:00</updated><id>http://localhost:4000/food_for_thought/week-6</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-6/">&lt;h3 id=&quot;monday-august-5-2019&quot;&gt;Monday August 5, 2019&lt;/h3&gt;

&lt;p&gt;I took last week off for the Primers Program at PPG in Pittsburgh (wow that was a lot of P’s) and I absolutely loved it! From Sunday to Thursday, we just learned about the details of the company, what they do (which is mostly make paint and coatings), and about the culture they create. Something I really liked was the emphasis on environmental impact as well as the well-being of its employees. No matter what your position, the company wants you to keep growing and learning and that’s something I can stand by.&lt;/p&gt;

&lt;p&gt;Alas, the vacation came to an end, and today I was back to work! Like I mentioned last week, we have a model working, but I want to continue making it more similar to the Smithsonian model and improve its accuracy. Today, I mainly worked on randomizing the training, validation, and test data (although I’m not sure how big of a difference it makes). Previously, I had 1600 images for training and validation, which between each other would get randomized, but the test data were always the same 200 images. This also required separate folders for the test data. Today, I pooled all the images I wanted to use into one folder and had the code split up the images. One thing I did run into at the end of the day, however, was when I went to use the model to predict the family, it threw an error. The format of the data is still a numpy array, but for some reason, the function isn’t accepting it. I used the debugger to compare what I previously had to what I currently have, and the only difference was that the numpy array was an element in a Python list. For tomorrow, I’ll try just putting the numpy array in a list and see if it works.&lt;/p&gt;

&lt;p&gt;Additionally, I attended presentations from the Women in Science interns at the Field Museum today! There were 5 groups of 2 working in various departments and it was really cool to see the hands-on science that they were doing! Very inspiring to see my peers work on such cool projects :)&lt;/p&gt;

&lt;p&gt;Just getting back into the swing of things!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-august-6-2019&quot;&gt;Tuesday August 6, 2019&lt;/h3&gt;

&lt;p&gt;This morning I came in and fixed my testing-the-model function. The numpy array I wanted to use had the dimensions 256 x 256 x 3, but the older version (which was working) uses an array with the dimensions 1 x 256 x 256 x 3. To fix it, I simply reshaped the numpy array to the desired dimensions. It’s crazy how such a simple fix like this could take so long to figure out! Afterwards, I worked on preventing overfitting with my model. One thing that I implemented today was L2 Regularization which essentially uses the squares of the coefficients as penalties in the loss function, emphasizing the impact of large coefficients in the model. So if the model consists of many large coefficients, they will quickly negatively impact the loss function. This prevents the loss function from decreasing too quickly and helps with overfitting of the model.&lt;/p&gt;

&lt;p&gt;In the Smithsonian model, they used L2, but it was applied to the entire model in one step. However, with Tensorflow, we only apply regularizers layer by layer. After looking at some examples online, it seems that most people apply them to the Dense layers, so I did as well, starting with an alpha value of 0.01. (Larger alpha values result in a larger loss). Usually the alpha value is between 0 and 1, but the Smithsonian model uses 5 which struck me as odd. That’s definitely something I would like to ask our NEIU grad student, Beth, with whom I’m meeting tomorrow.&lt;/p&gt;

&lt;p&gt;To recap, my model currently takes in all the pictures (for training, validation, and testing) in 2 folders (one for Lycopodiaceae and one for Selaginellaceae) and divides them accordingly. This way, we aren’t always testing on the same images and it gets randomized each time we re-train the model. However, sometimes we also want to test on images that aren’t originally input; in those cases, I have two scripts that can test the model: one takes from 2 folders on the computer and the other uses the images originally put in with the training and validation images.&lt;/p&gt;

&lt;p&gt;I feel like the model currently is pretty good, but my next steps, I would like to increase the number of epochs and maybe the number of images used to train the data in order to increase accuracy in validation and testing data. Honestly, I’m not really sure when you’re “done” training a model. On a last note, this Friday marks the end of the high school Digital Learning Internship that we’ve been helping out with and I’m going to do a quick presentation on this project! I’m excited to share the work I’ve done, even though it’s not super ground breaking or anything in the field of CS, and see the presentations from other interns as well!&lt;/p&gt;

&lt;p&gt;Oh also, this afternoon, we went for some dim sum lunch at a place in Old Chinatown called Triple Crown and it was &lt;span class=&quot;standOut&quot;&gt;AMAZING&lt;/span&gt;. 10/10 recommend! Then we had some time just to relax, walk around, and check out the Chinatown shops! What a realxing afternoon! :D&lt;/p&gt;

&lt;p&gt;Have a great day!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-august-7-2019&quot;&gt;Wednesday August 7, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I continued making changes to the model and seeing the result. Right before I left for my meeting at NEIU, I trained the model with more images (around 1600 training images) and tested. At first, with 180 test images, it had around a 97% success rate, but when I introduced 500 new images, it plummeted to around 50% success rate. I didn’t have time to examine why this happened, but I will tomorrow! If I can’t find a good reason why this is happening, I will go back to using less images and see if the model “reverts”.&lt;/p&gt;

&lt;p&gt;At my meeting with Beth, I mostly recapped what I had done and we talked about some steps moving forwards, which are listed below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the same images for training/validation because we want to see the performance of the model, we don’t want the model to be affected by variables like randomization of the images used between training/validation and testing.&lt;/li&gt;
  &lt;li&gt;Test that the models I’m saving still work. I copied the .model files into a different folder, but I didn’t take the h5 files. I figured since the testing files only called upon CNN.model, it should be fine, but I would like to test this.&lt;/li&gt;
  &lt;li&gt;Keep trying to improve model! Once we get something satisfactory, we can test on larger sets of images and change images used.&lt;/li&gt;
  &lt;li&gt;Look into using metadata to help with the classification (Ask Matt if this is something worth doing). Maybe knowing something like location of collection could help the model predict the specimen. However, this will take a bit more research!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lots to work on, so I’ll definitely be busy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-august-8-2019&quot;&gt;Thursday August 8, 2019&lt;/h3&gt;

&lt;p&gt;Okay not going to lie, this week has been a lot of fun! Yesterday was one of the interns, Amelia’s, birthday but she wasn’t here, so today, we went to Jewel Osco to get her a cake and celebrated! But for my project, when I used the model we trained for new images, it was the same probability as a coin toss. My hypothesis is as follows:&lt;/p&gt;

&lt;p&gt;When selecting the training images, I took the first say 1000 images from each folder. Then I tested the model on the next maybe 100 images after that. These images were probably kind of similar since they were just taken in order from the online collection which means they may be grouped by time they were collected or even location and collector. This could mean that the images at the beginning of each folder may be drastically different than the images from the end of the folder. So, when I went to go test the model on new images, it would result in accuracy of around 50%. Which is &lt;span class=&quot;standOut&quot;&gt;very bad&lt;/span&gt;. So my next step here is to work on a helper function that will take &lt;i&gt;all&lt;/i&gt; the specimen we have and choose images periodically to spread out the sample.&lt;/p&gt;

&lt;p&gt;After implementing this technique of choosing training images, I tested it on images that were chosen with a similar technique. It seems results are more consistent which is good, but of course our accuracy took a hit. Now I want to work on building it back up to what we had with the misleading accuracy. I also want to consider testing on randomly selected test images (that of course, were not seen during training). I honestly don’t think most of this is too hard, but the sheer amount of images that I’m dealing with is a little intimidating and keeping eveerything straight (which files are correct, separating old batches from new) is getting a little overwhelming. I might take a bit tomorrow to organize better. Also! Tomorrow is the high school interns’ last day :( They’ll be presenting their projects and I’m excited to hear about them!&lt;/p&gt;

&lt;p&gt;Let’s end this week strong!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-august-9-2019&quot;&gt;Friday August 9, 2019&lt;/h3&gt;

&lt;p&gt;In the morning, I really cracked down, working on my machine learning project. Right now, I’m trying to combat the issue of sometimes when I used the model on test data, it would only have 50% ish accuracy despite validation accuracy being really high. This is because the model is overfitting, so I’m trying to implement different methods to fight this. First, I implemented some early stopping. This will stop the model if the validation loss starts to increase again, although it’s not ideal. I need to first change some things about my model to fight the overfitting before just stopping training. Currently, validation accuracy will remain at about 50% while training accuracy increases then suddenly validation accuracy will increase and fluctuate a bit. My next steps will be to implement more dropouts in my model, but less percent of the model at a time. If that doesn’t really work, I may look into some image augmentation, which seems to be a popular suggestion online. (Sorry for the short entry today!)&lt;/p&gt;

&lt;p&gt;Enjoy the weekend!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday August 5, 2019</summary></entry><entry><title type="html">Week 5: Machine Learning Ramps Up</title><link href="http://localhost:4000/food_for_thought/FM-week-5/" rel="alternate" type="text/html" title="Week 5: Machine Learning Ramps Up" /><published>2019-07-22T06:25:40-07:00</published><updated>2019-07-22T06:25:40-07:00</updated><id>http://localhost:4000/food_for_thought/week-5</id><content type="html" xml:base="http://localhost:4000/food_for_thought/FM-week-5/">&lt;h3 id=&quot;monday-july-22-2019&quot;&gt;Monday July 22, 2019&lt;/h3&gt;

&lt;p&gt;Hi everyone! I can’t believe we’re already nearing the end of July! Is it just me or is this summer seriously flying by?? Anyways, today was major progress on the third part of my project: classification using machine learning! So just a quick recap, there’s an article by the Smithsonian explaining how they were able to use a ML algorithm to sort two plant families that are morphologically similar &lt;span class=&quot;standOut&quot;&gt;(Lycopodiaceae and Selaginellaceae)&lt;/span&gt;. Check it out &lt;a href=&quot;https://bdj.pensoft.net/article/21139/element/5/3811021/&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;here&lt;/a&gt;! They also used an algorithm to find stained specimen sheets, but for now, we’re only focused on the family part. My current goal is to understand machine learning enough to essentially replicate what they did in the article. Then after that, we would like to apply a similar concepts to other species or categories. For example, with the staining example done in the article, there’s a really practical purpose to conduct that. Essentially, in the past, specimen were treated with mercury to preserve them longer, but over time, we’ve learned that this is actually bad for the specimen and it causes staining on the specimen sheets. Eventually, we’d like to find the specimen that have been treated and are now stained so we can remount them, allowing them to last longer.&lt;/p&gt;

&lt;p&gt;Last week, I created scripts to do a lot of the initial image processing so all the pictures are the same size (256 pixels by 256 pixels) and have the same resolution (96 ppi). Today, I started creating my own ML model using tensorflow and keras to train a model, following the tutorial &lt;a href=&quot;https://towardsdatascience.com/all-the-steps-to-build-your-first-image-classifier-with-code-cf244b015799&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;here&lt;/a&gt; (bless the people who make easy to follow tutorials like this one!) Before I started attempting to recreate the model from the Smithsonian paper, I just wanted to get something working with the model in the tutorial and I did! I only used 100 images from each family and 10 epochs (epoch = essentially a cycle) and the accuracy is pretty trash, as you can see below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/initial_model_accuracy.png&quot; alt=&quot;Graph of accuracy of training model from tutorial. It's pretty trash&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that I have that, I plan to follow these steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Change the layers to recreate the model from the Smithsonian Paper&lt;/li&gt;
  &lt;li&gt;Increase my training size&lt;/li&gt;
  &lt;li&gt;Test the model on a test batch!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Starting on step 1, I was almost able to finish replicating the paper’s model in Python. Although the Smithsonian used Mathematica, with the help of Google (honestly, I don’t know what I’d do on this project without Google) I think I’m able to figure out what the paper’s layers are doing then finding the relative equivalent in Python. I’ve almost finished that and hope tomorrow I’ll be able to get a small batch tested with hopefully higher accuracy than the random one I used today. Something I would like to investigate further is how do we know how many layers or filters to use? What filter size should we use for the convolution layer? How did they figure out what combination of layers worked best? It seems that creating a model is not that difficult, it’s the &lt;i&gt;why&lt;/i&gt; you choose certain characteristics of the model that I’d like to further explore.&lt;/p&gt;

&lt;p&gt;Well today was a very satisfying day and tomorrow we have our Python workshop! Can’t wait :D&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;tuesday-july-23-2019&quot;&gt;Tuesday July 23, 2019&lt;/h3&gt;

&lt;p&gt;Hello! First thing I would like to talk about today is that I co-hosted my first Python workshop! It was for the other interns in the botany department as well as paleobotany and some high school interns. I think as a self reflection, overall it went pretty well! I think there was just a lot of information at once, especially for people who never used Python before. So in the future, if I were to do it again, I would be more careful about tailoring the workshop to the audience because of course everyone is at a different stage of learning! I sent out a feedback form, so hopefully that will help me become a better teacher in the future!&lt;/p&gt;

&lt;p&gt;More related to my project, I think I got a model that’s pretty similar to the one described in the Smithsonian paper. At least by what the layers are called/what I think they do. The difficult part in this was because the paper was done in Mathematica while I’m using TensorFlow in Python, so the types of layers/how to use them don’t carry over exactly. I’ve been doing a lot of research into what certain layers are intended to do in Mathematica and trying to find the Keras/TensorFlow equivalent for Python.&lt;/p&gt;

&lt;p&gt;With my first test, I used 100 images from each family (Lycopodiaceae and Selaginellaceae) to train/validate the model (10% of this batch are used for validation). The results of the accuracy are below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/training_100.png&quot; alt=&quot;Graphs of accuracy over epochs for model with training sample of 200 total images&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although the training accuracy increased over time, we see that by the second epoch, the validation accuracy plateaued, which is not a good sign. Additionally, when tested on the same test data as above, all the images were labeled as Selaginellaceae, leading me to believe overfitting is happening, but I can look into this more tomorrow.&lt;/p&gt;

&lt;p&gt;Exciting things are happening and I’m definitely kept busy!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;wednesday-july-24-2019&quot;&gt;Wednesday July 24, 2019&lt;/h3&gt;

&lt;p&gt;Today I took a bit of a break from the machine learning classification project. In the morning, a teacher from Northside College Prep came in and worked with us to create our own ozone detection strips! These strips can be used as indicators to see whether there are high levels of ozone in our area or not! They’re actually really easy to make; it was essentially mixing some cornstarch and water, add some sodium iodide, and painting them onto strips. Then we have to let them sit overnight.&lt;/p&gt;

&lt;p&gt;The majority of the afternoon was spent helping out some scientists and the high school Digital Learning Interns, but I did get to work on the CNN model a little bit. To combat the issues I faced yesterday, I decreased the learning rate of the model from the standard for Adam (0.001) to 0.0001 and that improved our accuracy for training and validation (see below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/decreased_lr.png&quot; alt=&quot;Graph of accuracy over epochs with learning rate at 0.0001&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks nice, but I believe the model is still overfitting to the training data. This hypothesis was supported when I tested the model on some test data with 10 images from each family and all of them were labeled as Lycopodiaceae (the opposite from last time). Online says this may be due to imbalance of input data in training, but I don’t think that’s the case because we have 800 of each family. Maybe not enough data? I still believe overfitting is an issue here, so we need to see how we can reduce that. It may be time to consult some experts.&lt;/p&gt;

&lt;p&gt;Although they’re tough, roadblocks are good! This means we’re learning :)&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thursday-july-25-2019&quot;&gt;Thursday July 25, 2019&lt;/h3&gt;

&lt;p&gt;Happy Thursday! In the morning, we did a bit work with the ozone strips we prepped yesterday. We took them outside for a bit and tomorrow morning we can scan them to see the ozone levels found. It was nice to just walk outside around the museum and appreciate the gardens and all, like this nice picture below!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/blog/fm/flower.jpg&quot; alt=&quot;Nice flower by museum&quot; class=&quot;images half&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the machine learning project, I noticed that overfitting was still occurring with our training model. Looking online, I found that Regularization could help. I did see &lt;span class=&quot;standOut&quot;&gt;L2 Regularization&lt;/span&gt; in the Smithsonian paper, but didn’t quite understand what was happening and didn’t implement it yet. The Smithsonian paper set the L2 Regularization to 5 which I found very odd because doing some research online, it seemed that regularization values were generally very small, less than 1. Additionally, in Mathematica, L2 Regularization is something set for the model as a whole, but in Keras, it’s done layer by layer. Keras also gives options for regularization in bias, activity, and kernel, so for now, I just created a new layer after the first Dense layer and set all the regularizers to 0.01.&lt;/p&gt;

&lt;p&gt;There is no better way to describe my experience right now than research, guess, and check.&lt;/p&gt;

&lt;p&gt;Additionally, I found you can add custom functions which would help match my dropout layer to the Smithsonian one. I found that the Mathematica dropout layer not only drops half the values, but also multiplies the remaining ones by 2, but dropout in Keras only drops a certain percent of the values. Creating a custom function, I used the basis of the relu activation function with two changes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I made the slope for negative values still 1, essentially making it an identity function&lt;/li&gt;
  &lt;li&gt;I multiplied the function by 2&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The model is running and saves, but when we want to load the model to test it, it’s unable to recognize my custom function which is something I’m still trying to figure out. In the end, I eventually took out the custom part because at this point, I’m not even able to test the model. Also, at the end of the day, I was training the model, but unfortunately left in a rush to catch my train. I’m not sure if this is normal, but I noticed that training the model takes up a LARGE majority of the computer’s memory and CPU, so that’s something I plan to look into tomorrow. I think before I continue tweaking and testing, it will be a good idea to update everyone from NEIU with what I’ve done (which may end up being completely useless) and just trying to learn more instead of just trying random things I find because that method is proving to be expensive (in matters of time)&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;friday-july-26-2019&quot;&gt;Friday July 26, 2019&lt;/h3&gt;

&lt;p&gt;First, I’d just like to say we need to make more of an effort as a community to create a better world. To use less resources, less energy, and think every day how wecan make a positive environmental impact on this world. It’s all about give and take, not just always taking. I for one am going to look into making even just small daily habit changes that can add up to make a much larger impact. Anyways, regarding my machine learning project, today I made a break through! I was struggling for the last few days because although the training and validation accuracies would be okay, when it came to the testing data, basically the model just associated one class with all the data, even when I shuffled the data before testing. Well, after spending hours and hours researching, today I finally realized why, and it’s actually quite a dumb reason, so drumroll please!&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;It’s because I didn’t scale the RGB values of the pixels in the testing data!!! Basically, for the training model, we scaled all the RGB values for each pixel (usually between 0 and 255) to be between 0 and 1, but I forgot to do the same for the training data! So &lt;i&gt;of course&lt;/i&gt; we wouldn’t be getting correct predictions!&lt;/p&gt;

&lt;p&gt;After making that change, things began working again and I was actually able to make predictions! Initially, without making changes to the initial model, I was able to get about 93% accuracy with my test data, and now I’m working on changing my model to have it more closely match the Smithsonian paper and seeing the effects. I started to see a little overfitting, so I’ve been increasing drop out and decreasing learning rate, for example, to combat that. I’ve also been keeping a &lt;a href=&quot;https://docs.google.com/spreadsheets/d/15972K_rdv3zpnvnV--wSaTpiK0jnVDkBzhUWjre5BLg/edit?usp=sharing&amp;quot;&quot; target=&quot;_blank&quot; class=&quot;standOut&quot;&gt;record of my changes and results&lt;/a&gt;. Check it out for an always up to date record!&lt;/p&gt;

&lt;p&gt;I won’t be in at all next week because I’m going to Pittsburgh, but this has been a great place to stop for myself!&lt;/p&gt;

&lt;p&gt;-Al&lt;/p&gt;</content><author><name></name></author><summary type="html">Monday July 22, 2019</summary></entry></feed>